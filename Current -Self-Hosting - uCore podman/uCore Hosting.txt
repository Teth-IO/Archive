+-------------------------------------------------------------------------+
|=-------------------=[ Immutable hosting platform ]=--------------------=|
|-------------------------------------------------------------------------|
|=----------=[ uCore, Podman Quadlet rootless unit, RAID ZFS ]=----------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation
	1.1 - rebase ucore
	1.2 - maj auto
2 - RAID ZFS
	2.1 - SecureBoot avec kmod ZFS
	2.2 - pool mirroir
	2.3 - dataset
	2.4 - snapshot journalière sanoid
	2.5 - systemd timers
3 - Podman
  	3.1 - port commun en rootless
	3.2 - podman socket activation
	3.3 - firewalld
	3.4 - Network
	3.5 - démarrage auto en rootless
3 - Searxng
5 - owncloud infini scale (oCIS)
	6.1 - Infinite Scale Setup
	6.2 - Infinite Scale deploiement
6 - navidrome
7 - homepage
	7.1 - settings
	7.2 - bookmarks
	7.3 - services
	7.4 - widgets
	7.5 - docker.yaml
	7.6 - docker run
8 - traefik
   	8.1 - SSl
		8.1.1 - CA
		8.1.2 - Certificates
    	8.2 - config
    	8.3 - quadlet

--[ 0 - Introduction

# Fedora CoreOS (FCOS) rebase en uCore
# Immutable, atomic auto update, secure (SELinux + SecureBoot + immutable)
# uCore-ZFS avec module signé et batteries included (podman, docker, sanoid, firewalld, cockpit ...)
# data sous un RAID ZFS avec sbapshot journalière
# hosting de diférent container sous podman quadlet en rootless
# services accessible par traefik avec routage par domaine + tls
# services : searxng, owncloud ocis, navidrome, homepage

--[ 1 - installation

----[ 1.1 - rebase ucore

# créer un fichier butane qui comprend =  clef ssh, conf réseaux, maj et reboot auto + rebase uCore en stable-zfs
-> https://github.com/ublue-os/ucore?tab=readme-ov-file#auto-rebase-install + https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/
butane --pretty --strict .\truc.bu > .\truc.ign

mettre le .ign sur une clef usb et depuis le live environnement
(hyper-v mkfs.vfat sur le vhdx puis copy/paste)
sudo mount /dev/sda /mnt
cp /mnt/truc.ign ~/truc.ign
sudo umount /mnt

sudo coreos-installer install /dev/sda --ignition-file /truc.ign
# ou
sudo coreos-installer install /dev/sda  --ignition-url https://raw.githubusercontent.com/Teth-IO/infra/refs/heads/main/uCore%20-%20NAS/truc.ign

----[ 1.2 - maj auto

sudo systemctl edit --force --full rpm-ostreed-upgrade-reboot.service

'
# workaround for missing reboot policy
# https://github.com/coreos/rpm-ostree/issues/2843
[Unit]
Description=rpm-ostree upgrade and reboot
ConditionPathExists=/run/ostree-booted

[Service]
Type=simple
ExecStart=/usr/bin/rpm-ostree upgrade --reboot
#StandardOutput=null
Restart=always
RestartSec=60s
'

sudo bash -c 'cat << EOF > /etc/systemd/system/rpm-ostreed-upgrade-reboot.timer
[Unit]
Description=rpm-ostree upgrade and reboot trigger
ConditionPathExists=/run/ostree-booted

[Timer]
OnCalendar=*-*-* 4:00:00

[Install]
WantedBy=timers.target
EOF
'

sudo systemctl daemon-reload
sudo systemctl enable rpm-ostreed-upgrade-reboot.timer
sudo systemctl start rpm-ostreed-upgrade-reboot.timer

--[ 2 - RAID ZFS

# création du RAID1 par zfs
# snapshot journalière par sanoid avec 30 jours de rétention

----[ 2.1 - SecureBoot avec kmod ZFS

# ZFS ne marchera pas par défaut sous secureboot avant d'avoir importer la clef publique en machine-owner key (MOK)

sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der
-> demande de saisir un mot de passe, sera demander au prochain démarrage pour enregistrer la MOK

----[ 2.2 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)

----[ 2.3 - dataset

# un pool est volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

----[ 2.4 - snapshot journalière sanoid

sudo bash -c 'cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF
'

----[ 2.5 - systemd timers

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF
'

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF
'

sudo systemctl daemon-reload
sudo systemctl enable sanoid.timer
sudo systemctl start sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

--[ 3 - podman 

# maj auto des container avec AutoUpdate=registry dans les quadlet
systemctl --user enable --now podman-auto-update

sudo systemctl enable -now podman 

# attention au nom canonique de l'image (searxng vs searxng/searxng)
# bien se référer à la doc

# commes les containers seront sous des systemd unit en user il faut autoriser le lingering de celui-ci sinon les services tomberont avec la déco du compte
loginctl enable-linger core

----[ 3.1 - port commun en rootless

# pour port commum utilisable en rootless
echo "net.ipv4.ip_unprivileged_port_start=443" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

----[ 3.2 - podman socket activation

systemctl --user enable --now podman.socket

----[ 3.3 - firewalld

# a l'inverse de docker, pas whitelisté par defaut (zone docker active)
# besoin que du port 443, traefik route en backend en fonction du domaine

sudo firewall-cmd --add-port=443/tcp --permanent
sudo firewall-cmd --reload

----[ 3.4 - Network

# besoin de créer un nouveau réseau car celui par défaut est limité (pas d'ip) en rootless (passe de slirp4netns à netavark)
https://github.com/containers/podman/blob/main/docs/tutorials/basic_networking.md#basic-network-setups

podman network create backend

----[ 3.5 - démarrage auto en rootless

# les quadlet arrivait à redémarrer automatiquement après un redémarrage mais pas sous bare-metal ?
# créer un service dégeu qui fixe ça :

cat << EOF > ~/.config/systemd/user/podman-restart.service
[Unit]
Description=Podman Start All Containers With Restart Policy Set To Always
Documentation=man:podman-start(1)
StartLimitIntervalSec=0
Wants=network-online.target
After=network-online.target

[Service]
Type=oneshot
RemainAfterExit=true
Environment=LOGGING="--log-level=info"
ExecStart=/usr/bin/podman $LOGGING start --all --filter restart-policy=always
ExecStop=/bin/sh -c '/usr/bin/podman $LOGGING stop $(/usr/bin/podman container ls --filter restart-policy=always -q)'

[Install]
WantedBy=default.target
EOF

--[ 4 - searxng

# gluetun a besoin d'être priviligier pour /dev/net/tun https://github.com/qdm12/gluetun-wiki/blob/main/errors/tun.md

mkdir -p ~/.config/containers/systemd/

cat << EOF > ~/.config/containers/systemd/searxng.container
[Unit]
Description=searxng container
After=gluetun.service

[Container]
Image=docker.io/searxng/searxng:latest
AutoUpdate=registry
Network=container:systemd-gluetun

[Service]
Restart=always

[Install]
WantedBy=multi-user.target default.target
EOF

cat << EOF > ~/.config/containers/systemd/gluetun.container
[Unit]
Description=gluetun container

[Container]
Image=docker.io/qmcgaw/gluetun:latest
AutoUpdate=registry
AddCapability=NET_ADMIN
Environment=VPN_SERVICE_PROVIDER=custom
Environment=VPN_TYPE=wireguard
Environment=WIREGUARD_ENDPOINT_IP=redacted
Environment=WIREGUARD_ENDPOINT_PORT=redacted
Environment=WIREGUARD_PUBLIC_KEY=redacted
Environment=WIREGUARD_PRIVATE_KEY=redacted
Environment=WIREGUARD_ADDRESSES=redacted
AddDevice=/dev/net/tun:/dev/net/tun
PodmanArgs=--privileged
Label=traefik.http.routers.searxng.rule=Host(\`searxng.lan\`)
Label=traefik.http.services.searxng.loadbalancer.server.port=8080
Label=traefik.http.routers.searxng.tls=true
Network=backend

[Service]
Restart=always

[Install]
WantedBy=multi-user.target default.target
EOF

systemctl --user daemon-reload
systemctl --user start gluetun searxng
 
--[ 5 - owncloud infini scale (oCIS)

# owncloud version cloud-native sous micro-services (plus de php, sql...)
https://doc.owncloud.com/ocis/next/deployment/container/container-setup.html

----[ 5.1 - Infinite Scale setup

# utiliser les volumes au lieu des mount car ces derniers ne supportent pas les labels selinux

mkdir -p /mnt/raid/nas/ocis/ocis-config
mkdir /mnt/raid/nas/ocis/ocis-data

# besoin de podman unshare pour modifier le propritaire de /etc/ocis et /var/lib/ocis dans le namespace

podman unshare chown -R 1000:1000 /mnt/raid/nas/ocis/ocis-config
podman unshare chown -R 1000:1000 /mnt/raid/nas/ocis/ocis-data

podman run --rm -it \
    --volume /mnt/raid/nas/ocis/ocis-config:/etc/ocis:Z \
    owncloud/ocis init

----[ 5.2 - Infinite Scale deploiement

cat << EOF > ~/.config/containers/systemd/ocis.container
[Unit]
Description=ocis container

[Container]
Image=docker.io/owncloud/ocis:latest
AutoUpdate=registry
Volume=/mnt/raid/nas/ocis/ocis-config:/etc/ocis:Z
Volume=/mnt/raid/nas/ocis/ocis-data:/var/lib/ocis:Z
Environment=OCIS_INSECURE=true
Environment=PROXY_TLS=false
Environment=OCIS_URL=https://owncloud.lan
Label=traefik.http.routers.ocis.rule=Host(\`owncloud.lan\`)
Label=traefik.http.services.ocis.loadbalancer.server.port=9200
Label=traefik.http.routers.ocis.tls=true
Network=backend

[Service]
Restart=always

[Install]
WantedBy=multi-user.target default.target
EOF

systemctl --user daemon-reload
systemctl --user start ocis

pour exécuter les commandes d'admin :

docker exec -it <container-id> sh
ocis --help

--[ 6 - navidrome

mkdir /mnt/raid/nas/music/albums
mkdir /mnt/raid/nas/music/conf

cat << EOF > ~/.config/containers/systemd/navidrome.container
[Unit]
Description=navidrome container

[Container]
Image=docker.io/deluan/navidrome:latest
AutoUpdate=registry
Volume=/mnt/raid/nas/music/albums:/music:Z
Volume=/mnt/raid/nas/music/conf:/data:Z
Label=traefik.http.routers.navidrome.rule=Host(\`mytube.lan\`)
Label=traefik.http.services.navidrome.loadbalancer.server.port=4533
Label=traefik.http.routers.navidrome.tls=true
Network=backend

[Service]
Restart=always

[Install]
WantedBy=multi-user.target default.target
EOF

systemctl --user daemon-reload
systemctl --user start navidrome

--[ 7 - homepage

# Pour afficher les stats et services de l'intranet

mkdir -p /mnt/raid/nas/homepage/icon
cd /mnt/raid/nas/homepage/icon
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/openwrt.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/owncloud.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/searxng.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/gluetun.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/traefik.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/homepage.png
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/navidrome.svg

----[ 7.1 - settings

cat << EOF > /mnt/raid/nas/homepage/settings.yaml
---
background:
  image: https://images3.alphacoders.com/132/1324687.jpg
  brightness: 50
  saturate: 50
color: slate
layout:
  Podman:
    style: row
    columns: 3
  OpenWRT:
    Upstream:
      style: row
      columns: 3
    Info:
      style: row
      columns: 2
showStats: true
EOF

----[ 7.2 - bookmarks

cat << EOF > /mnt/raid/nas/homepage/bookmarks.yaml
---
- Intranet:
    - ownCloud:
        - icon: owncloud.svg
          href: https://owncloud.lan
    - OpenWRT:
        - icon: openwrt.svg
          href: https://192.168.1.1
    - SearXNG:
        - icon: searxng.svg
          href: https://searxng.lan
    - Navidrone:
        - icon: navidrome.svg
          href: https://mytube.lan
EOF

----[ 7.3 - services

cat << EOF > /mnt/raid/nas/homepage/services.yaml
---
- OpenWRT:
    - Upstream:
        icon: openwrt.svg
        widget:
          type: openwrt
          url: https://192.168.1.1
          username: redacted
          password: redacted
          interfaceName: eth1
    - Info:
        icon: openwrt.svg
        widget:
          type: openwrt
          url: https://192.168.1.1
          username: redacted
          password: redacted

- Podman:
    - Traefik:
        icon: traefik.svg
        description: reverse-proxy pour routage et tls
        server: my-pods
        container: systemd-traefik
    - SearXNG:
        icon: searxng.svg
        description: meta-moteur de recherche
        href: "https://https://searxng.lan"
        server: my-pods
        container: systemd-searxng
    - Gluetun:
        icon: gluetun.svg
        description: proxy wireguard pour container
        server: my-pods
        container: systemd-gluetun
    - OCIS:
        icon: owncloud.svg
        description: partage de fichier
        href: "https://owncloud.lan"
        server: my-pods
        container: systemd-ocis
    - homepage:
        icon: homepage.png
        description: intranet
        href: "https://homepage.lan"
        server: my-pods
        container: systemd-homepage
    - navidrome:
        icon: navidrome.svg
        description: serveur de musique
        href: "https:mytube.lan"
        server: my-pods
        container: systemd-navidrome
EOF

----[ 7.4 - widgets

cat << EOF > /mnt/raid/nas/homepage/widgets.yaml
---
- resources:
    cpu: true
    memory: true
    cputemp: true
    tempmin: 0 # optional, minimum cpu temp
    tempmax: 100 # optional, maximum cpu temp
    uptime: true
    refresh: 3000 # optional, in ms
EOF

----[ 7.5 - docker yaml

cat << EOF > /mnt/raid/nas/homepage/docker.yaml
---
my-pods:
  socket: /var/run/docker.sock
EOF

----[ 7.6 - quadlet

# PUID, PGID et securitylabel pour lire le socket podman en rootless

cat << EOF > ~/.config/containers/systemd/homepage.container
[Unit]
Description=homepage container

[Container]
Image=ghcr.io/gethomepage/homepage:latest
AutoUpdate=registry
Volume=/mnt/raid/nas/homepage:/app/config:Z
Volume=/run/user/1000/podman/podman.sock:/var/run/docker.sock:Z
Volume=/mnt/raid/nas/homepage/icon:/app/public/icons:Z
Label=traefik.http.routers.homepage.rule=Host(\`homepage.lan\`)
Label=traefik.http.services.homepage.loadbalancer.server.port=3000
Label=traefik.http.routers.homepage.tls=true
Network=backend
SecurityLabelType=container_runtime_t
Environment=PUID: 1000
Environment=PGID: 1000

[Service]
Restart=always

[Install]
WantedBy=multi-user.target default.target
EOF

systemctl --user daemon-reload
systemctl --user start homepage

--[ 8 - traefik

# traefik se configure soit avec un fichier de conf, soit avec des commandes, soit avec des variables d'env
# choisir une des méthodes, traefik ne supporte dans utiliser plusieurs en même temps

----[ 8.1 - SSl

------[ 8.1.1 - CA

# créer la key
openssl genrsa -out lan.key 4096
# creér le root certificate (le common name indique le domaine sous lequel on signe, ici lan)
# le root certificate est à installer sur toutes les machines qui ont besoin d'accéder aux services lan
openssl req -x509 -new -nodes -key lan.key -sha256 -days 3650 -out lan.pem -subj "/CN=lan"

------[ 8.1.2 - certificates

## homepage
# créer la clef privé du website
openssl genrsa -out homepage.lan.key 4096
# créer le csr (common name = server hostname = homepage.lan)
openssl req -new -key homepage.lan.key -out homepage.lan.csr -subj "/CN=homepage.lan"
# extension X509 V3
cat << EOF > homepage.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = homepage.lan
EOF
# le certificat final
openssl x509 -req -in homepage.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out homepage.lan.crt -days 3650 -sha256 -extfile homepage.lan.ext

## owncloud
openssl genrsa -out owncloud.lan.key 4096
openssl req -new -key owncloud.lan.key -out owncloud.lan.csr  -subj "/CN=owncloud.lan"
cat << EOF > owncloud.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = owncloud.lan
EOF
openssl x509 -req -in owncloud.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out owncloud.lan.crt -days 3650 -sha256 -extfile owncloud.lan.ext

## navidrome
openssl genrsa -out mytube.lan.key 4096
openssl req -new -key mytube.lan.key -out mytube.lan.csr  -subj "/CN=mytube.lan"
cat << EOF > mytube.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = mytube.lan
EOF
openssl x509 -req -in mytube.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out mytube.lan.crt -days 3650 -sha256 -extfile mytube.lan.ext

## searxng
openssl genrsa -out searxng.lan.key 4096
openssl req -new -key searxng.lan.key -out searxng.lan.csr  -subj "/CN=searxng.lan"
cat << EOF > searxng.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = searxng.lan
EOF
openssl x509 -req -in searxng.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out searxng.lan.crt -days 3650 -sha256 -extfile searxng.lan.ext

----[ 8.2 - config 

mkdir /mnt/raid/nas/traefik

# indique la config dynamique pour les certs
cat << EOF > /mnt/raid/nas/traefik/traefik.yml
entryPoints:
  websecure:
    address: ":443"

providers:
  docker: {}
  file:
    directory: /home/appuser/data
    watch: true
EOF

# liste les certs à utiliser, le nom doit correspondre au domaine
cat << EOF > /mnt/raid/nas/ssl/tls.yml
tls:
  certificates:
    - certFile: /home/appuser/data/searxng.lan.crt
      keyFile: /home/appuser/data/searxng.lan.key
    - certFile: /home/appuser/data/owncloud.lan.crt
      keyFile: /home/appuser/data/owncloud.lan.key
    - certFile: /home/appuser/data/homepage.lan.crt
      keyFile: /home/appuser/data/homepage.lan.key
    - certFile: /home/appuser/data/mytube.lan.crt
      keyFile: /home/appuser/data/mytube.lan.key
EOF

----[ 8.3 - quadlet

cat << EOF > ~/.config/containers/systemd/traefik.container
[Unit]
Description=traefik container

[Container]
Image=docker.io/traefik:latest
AutoUpdate=registry
PublishPort=443:443
Volume=/run/user/1000/podman/podman.sock:/var/run/docker.sock:z
Volume=/mnt/raid/nas/traefik/traefik.yml:/etc/traefik/traefik.yml:z
Volume=/mnt/raid/nas/ssl:/home/appuser/data:Z
SecurityLabelType=container_runtime_t
Network=backend

[Service]
Restart=always

[Install]
WantedBy=multi-user.target default.target
EOF

systemctl --user daemon-reload
systemctl --user start traefik

+-[ EOF ]=----------------------------------------------------------------+
