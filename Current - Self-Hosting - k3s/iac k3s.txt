+-------------------------------------------------------------------------+
|=-------------------=[ Immutable hosting platform ]=--------------------=|
|-------------------------------------------------------------------------|
|=----------=[ uCore, Podman Quadlet rootless unit, RAID ZFS ]=----------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation
    1.1 - rebase uCore
    1.2 - k3s
    1.3 - Flux CD
    1.4 - Helm
    1.5 - stockage
        1.5.1 - SecureBoot avec kmod ZFS
        1.5.2 - pool mirroir
        1.5.3 - dataset
        1.5.4 - snapshot 
            1.5.4.1 - service et timer
        1.5.5 - backup
            1.5.5.1 - service et timer
    1.6 - redis
    1.7 - maintenance
        1.7.1 - du server en maj auto
        1.7.2 - des pods avec flux
        1.7.3 - du cluster
2 - apps
    2.0 - Certificat Authority
    2.1 - Searxng
    2.2 - owncloud infini scale (oCIS)
    2.3 - navidrome
    2.4 - headlamp
    2.5 - paperless-ngx
    2.6 - calibre-web
    2.7 - jellyfin

--[ 0 - Introduction

--[ 1 - installation

core os rebase en ucore
uCore car install k3s fonctionnel en une commande
comprend sanoid et tailscale par défaut
zfs avec secureboot

----[ 1.1 - rebase ucore

# créer un fichier butane qui comprend =  clef ssh, conf réseaux, maj et reboot auto + rebase uCore en stable-zfs
-> https://github.com/ublue-os/ucore?tab=readme-ov-file#auto-rebase-install + https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/
butane --pretty --strict .\truc.bu > .\truc.ign

wget le fichier ignition
sudo coreos-installer install /dev/sda --ignition-file /truc.ign

!! bien mettre un hostname à la machine
=> à spec dans ign
!! les noeuds sous k8S sont identifier par nom de domaine
!! si ignoré la machine se l'attribut par dns
!! risque de récupérer le nom d'un service hebergé, risque de changement de nom de domaine par la suite ce qui entrainer une fausse addition de noeud et une migration des pods

!! dns, les app peuvent vouloir résoudre leur hostname (surtout owncloud)
pb kube-dns ignore le fichier hosts et ici on utilise nextdns (plus de serveur dns interne à l'infra) qui marche que sur DOH

tailscale up
tailscale ip = 100.102.192.19

----[ 1.2 - k3s

curl -sfL https://get.k3s.io | sh -

# intègre traefik pour l'ingress
# traefik peut utiliser une custom resource definition (CRD), l'ingressroute

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

# doit modif les infos tls du cert pour autoriser les co depuis l'int tailscale

cat << EOF > /etc/rancher/k3s/config.yaml
tls-san: 
  - 100.102.192.19
EOF

kubectl -n kube-system delete secrets/k3s-serving
mv /var/lib/rancher/k3s/server/tls/dynamic-cert.json /tmp/dynamic-cert.json

systemctl restart k3s

----[ 1.3 - FluxCD

# heberge les manifest sous git
# flux reconcilie le cluster avec l'état attendu sous git
# monitoring des repository avec maj auto

curl -s https://fluxcd.io/install.sh | sudo bash
. <(flux completion bash)
cp  /etc/rancher/k3s/k3s.yaml ~/.kube/config
flux check --pre

# bootstrap GitHub Personal Account avec les components pour l'automation
export GITHUB_TOKEN=<your-token>
export GITHUB_USER=<your-username>
flux bootstrap github \
  --components-extra=image-reflector-controller,image-automation-controller \
  --owner=$GITHUB_USER \
  --repository=k3s \
  --branch=main \
  --path=clusters/my-cluster \
  --read-write-key \
  --personal

cloner le repo
mettre les manifest dans /clusters/my-cluster
  attention : les manifests doivent spécifier le namespace à utiliser sinon timeout
push :
git add -A && \
git commit -m "add apps" && \
git push origin main

maj auto, possible de pull manuellement :
# flux reconcile kustomization flux-system --with-source

----[ 1.4 - Helm

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
=> évidemment ne marche pas car fs read only
-> utiliser le binaire au pif du coup

----[ 1.5 - stockage

longhorn et rook sont du stockage distribuer sous hyperconvergeance
=> besoin de plusieurs noeuds, répartissent les données d'eux mêmes, pas la main sur leur positions
n'agira pas comme un remplaçant pour le raid

=> zfs toujours dispo, inclus dans les extansions de flatcar
sanoid et syncoid sont des scripts à skip donc toujours dispo
migration entre machine facilité avec syncoid

=> voir activation du sysext pour le ZFS
=> voir ce qu'il en est du future secure boot de flatcar avec le module de kernel

------[ 1.5.1 - SecureBoot avec kmod ZFS

# ZFS ne marchera pas par défaut sous secureboot avant d'avoir importer la clef publique en machine-owner key (MOK)

sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der
-> demande de saisir un mot de passe, sera demander au prochain démarrage pour enregistrer la MOK

------[ 1.5.2 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)
=> ne pause pas de pb pour les bdd en terme de cow comme le recodsize du fs est bien supérieurà celui qu'elles utilises

------[ 1.5.3 - dataset

# un pool est volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

------[ 1.5.4 - snapshot

cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF

--------[ 1.5.4.1 - service et timer

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF

cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable sanoid.timer
systemctl start sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

------[ 1.5.5 - backup

restic vers s3
passage vers glacier si coute trop chère

céer un compte de service restic sur aws avec les bonnes stratégie
https://restic.readthedocs.io/en/latest/080_examples.html

export AWS_DEFAULT_REGION="redacted"
export RESTIC_REPOSITORY="redacted"
export AWS_ACCESS_KEY_ID="redacted"
export AWS_SECRET_ACCESS_KEY="redacted"
export RESTIC_PASSWORD="redacted"

# initie le dépot
restic init

# première backup
restic backup /mnt/raid/nas/

--------[ 1.5.5.1 - service et timer

=> à automatiser

cat << EOF > backup.sh
#!/bin/bash
export AWS_DEFAULT_REGION="redacted"
export RESTIC_REPOSITORY="redacted"
export AWS_ACCESS_KEY_ID="redacted"
export AWS_SECRET_ACCESS_KEY="redacted
export RESTIC_PASSWORD="redacted"
/home/core/restic backup /mnt/raid/nas/
/home/core/restic forget --keep-last 10
/home/core/restic prune
EOF

cat << EOF > /etc/systemd/system/restic.service

EOF

cat << EOF > /etc/systemd/system/restic.timer

EOF

----[ 1.6 - redis

# dragonfly de dispo en remplacement de redis mais ne marche pas.
# pb de hardware ?
# à l'inverse de postgres ici en monolithique comme dragonfly ne traite aucune donnée critique
# operateur de dispo mais osef du HA

# dragonfly semble mieux mais pb avec le cpu

----[ 1.7 - maintenance

------[ 1.7.1 - du serveur sous maj auto

systemctl edit --force --full rpm-ostreed-upgrade-reboot.service

'
# workaround for missing reboot policy
# https://github.com/coreos/rpm-ostree/issues/2843
[Unit]
Description=rpm-ostree upgrade and reboot
ConditionPathExists=/run/ostree-booted

[Service]
Type=simple
ExecStart=/usr/bin/rpm-ostree upgrade --reboot
#StandardOutput=null
Restart=always
RestartSec=60s
'

cat << EOF > /etc/systemd/system/rpm-ostreed-upgrade-reboot.timer
[Unit]
Description=rpm-ostree upgrade and reboot trigger
ConditionPathExists=/run/ostree-booted

[Timer]
OnCalendar=*-*-* 4:00:00

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable rpm-ostreed-upgrade-reboot.timer
systemctl start rpm-ostreed-upgrade-reboot.timer

reboot

------[ 1.7.1 - des pods

images en latest
=> marche pour certaines dépôt (quay, docker)
flux image update automation pour le reste (ghcr)
=> ne marche que sur du semver

## flux image update automation
# pour chaque image on peut créer un job de scan, exemple :
flux create image repository podinfo \
--image=ghcr.io/stefanprodan/podinfo \
--interval=60m \
--export > ./podinfo-registry.yaml

# on désigne ensuite la stratégie à utiliser pour filtrer ces tags (que les patch, ques les versions mineurs, inclure les pre-release...)
flux create image policy podinfo \
--image-ref=podinfo \
--select-semver='>=1.0.0' \
--export > ./podinfo-policy.yaml

# on créer ensuite imageupdateautomation pour mettre à jour les manifest lors de la detection des nouvelles images
flux create image update flux-system \
--interval=60m \
--git-repo-ref=flux-system \
--git-repo-path="./clusters/my-cluster" \
--checkout-branch=main \
--push-branch=main \
--author-name=fluxcdbot \
--author-email=fluxcdbot@users.noreply.github.com \
--commit-template="{{range .Changed.Changes}}{{print .OldValue}} -> {{println .NewValue}}{{end}}" \
--export > ./flux-system-automation.yaml


voir rennovate si besoin
https://www.mend.io/renovate/

------[ 1.7.2 - du cluster

https://docs.k3s.io/upgrades
# k3s intègre une custom resource definition (CRD), les plans
# apparemment monitor et maj auto
prendre les plans server et agents
remplacer version par le channel field
mettre en latest

--[ 2 - apps

----[ 2.0 - Certificat Authority

# créer la key
openssl genrsa -out lan.key 4096
# creér le root certificate (le common name indique le domaine sous lequel on signe, ici lan)
# le root certificate est à installer sur toutes les machines qui ont besoin d'accéder aux services lan
openssl req -x509 -new -nodes -key lan.key -sha256 -days 3650 -out lan.pem -subj "/CN=lan"

----[ 2.1 - searxng

# créer la clef privé du website
openssl genrsa -out searxng.lan.key 4096

# créer le csr (common name = server hostname = homepage.lan)
openssl req -new -key searxng.lan.key -out searxng.lan.csr  -subj "/CN=searxng.lan"

# extension X509 V3
cat << EOF > searxng.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = searxng.lan
EOF

# le certificat final
openssl x509 -req -in searxng.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out searxng.lan.crt -days 3650 -sha256 -extfile searxng.lan.ext

# le secret :
sudo kubectl create secret tls searxng-secret \
  --cert=./searxng.lan.crt \
  --key=./searxng.lan.key

----[ 2.2 - ocis

openssl genrsa -out owncloud.lan.key 4096

openssl req -new -key owncloud.lan.key -out owncloud.lan.csr  -subj "/CN=owncloud.lan"

cat << EOF > owncloud.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = owncloud.lan
EOF

openssl x509 -req -in owncloud.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out owncloud.lan.crt -days 3650 -sha256 -extfile owncloud.lan.ext

sudo kubectl create secret tls owncloud-secret \
  --cert=./owncloud.lan.crt \
  --key=./owncloud.lan.key

----[ 2.3 - navidrome

openssl genrsa -out mytube.lan.key 4096

openssl req -new -key mytube.lan.key -out mytube.lan.csr  -subj "/CN=mytube.lan"

cat << EOF > mytube.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = mytube.lan
EOF

openssl x509 -req -in mytube.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out mytube.lan.crt -days 3650 -sha256 -extfile mytube.lan.ext

sudo kubectl create secret tls navidrome-secret \
  --cert=./mytube.lan.crt \
  --key=./mytube.lan.key

----[ 2.4 - headlamp

openssl genrsa -out homepage.lan.key 4096

openssl req -new -key homepage.lan.key -out homepage.lan.csr -subj "/CN=homepage.lan"

cat << EOF > homepage.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = homepage.lan
EOF

openssl x509 -req -in homepage.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out homepage.lan.crt -days 3650 -sha256 -extfile homepage.lan.ext

sudo kubectl create secret tls homepage-secret \
  --cert=./homepage.lan.crt \
  --key=./homepage.lan.key

# créer le compte de service
https://headlamp.dev/docs/latest/installation/#create-a-service-account-token

----[ 2.5 - paperless-ngx

openssl genrsa -out paperless.lan.key 4096

openssl req -new -key paperless.lan.key -out paperless.lan.csr -subj "/CN=paperless.lan"

cat << EOF > paperless.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = paperless.lan
EOF

openssl x509 -req -in paperless.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out paperless.lan.crt -days 3650 -sha256 -extfile paperless.lan.ext

sudo kubectl create secret tls paperless-secret \
  --cert=./paperless.lan.crt \
  --key=./paperless.lan.key

# créer le compte admin
python3 manage.py createsuperuser

----[ 2.6 - calibre-web-automated

openssl genrsa -out calibre.lan.key 4096

openssl req -new -key calibre.lan.key -out calibre.lan.csr -subj "/CN=calibre.lan"

cat << EOF > calibre.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = calibre.lan
EOF

openssl x509 -req -in calibre.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out calibre.lan.crt -days 3650 -sha256 -extfile calibre.lan.ext

sudo kubectl create secret tls calibre-secret \
  --cert=./calibre.lan.crt \
  --key=./calibre.lan.key

----{ 2.7 - jellyfin

openssl genrsa -out jellyfin.lan.key 4096

openssl req -new -key jellyfin.lan.key -out jellyfin.lan.csr  -subj "/CN=jellyfin.lan"

cat << EOF > jellyfin.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = jellyfin.lan
EOF

openssl x509 -req -in jellyfin.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out jellyfin.lan.crt -days 3650 -sha256 -extfile jellyfin.lan.ext

sudo kubectl create secret tls jellyfin-secret \
  --cert=./jellyfin.lan.crt \
  --key=./jellyfin.lan.key

----------------- notes

logs :
sudo kubectl logs pod-name

shell :
sudo kubectl exec --stdin --tty pod -- /bin/sh

# toutes les ressources
kubectl get 

# voir le détails des tag d'images observé
kubectl -n flux-system describe imagerepositories searxng

# voir le status du traking des images
flux get images all --all-namespaces

# debug les reconciliation
flux get ks

flux reconcile kustomization flux-system --with-source

=> pb de node unschedulable
kubectl patch nodes searxng.lan --patch '{"spec":{"unschedulable": false}}'