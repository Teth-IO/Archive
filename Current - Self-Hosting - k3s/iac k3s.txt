+-------------------------------------------------------------------------+
|=-------------------=[ Immutable hosting platform ]=--------------------=|
|-------------------------------------------------------------------------|
|=----------=[ uCore, Podman Quadlet rootless unit, RAID ZFS ]=----------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation
    1.1 - flatcar
    1.2 - k3s
    1.3 - Flux CD
    1.4 - Helm
    1.5 - stockage
        1.5.1 - pool mirroir
        1.5.2 - dataset
        1.5.3 - snapshot 
        1.5.4 - timers
    1.6 - renovate
    1.7 - CloudNativePG
    1.8 - redis
    1.9 - maintenance
2 - apps
    2.0 - CA
    2.1 - Searxng
    2.2 - owncloud infini scale (oCIS)
    2.3 - navidrome
    2.4 - homepage
    2.5 - paperless-ngx
    2.6 - calibre-web
    2.7 - jellyfin

--[ 0 - Introduction

--[ 1 - installation

----[ 1.1 - flatcar

wget le fichier ignition
flatcar-install -d /dev/sda -i ignition.json

première co :

systemctl enable --now fstrim.timer
/etc/selinux/config -> enforcing

reboot

----[ 1.2 - k3s 

curl -sfL https://get.k3s.io | sh -

# intègre traefik pour l'ingress
# traefik peut utiliser une custom resource definition (CRD), l'ingressroute

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

----[ 1.3 - FluxCD

# heberge les manifest sous git
# flux reconcilie le cluster avec l'état attendu sous git
# monitoring des repository avec maj auto

curl -s https://fluxcd.io/install.sh | sudo bash
. <(flux completion bash)
cp  /etc/rancher/k3s/k3s.yaml ~/.kube/config
flux check --pre

# bootstrap GitHub Personal Account avec les components pour l'automation
export GITHUB_TOKEN=<your-token>
export GITHUB_USER=<your-username>
flux bootstrap github \
  --components-extra=image-reflector-controller,image-automation-controller \
  --owner=$GITHUB_USER \
  --repository=k3s \
  --branch=main \
  --path=clusters/my-cluster \
  --read-write-key \
  --personal

cloner le repo
mettre les manifest dans /clusters/my-cluster
  attention : les manifests doivent spécifier le namespace à utiliser sinon timeout
push :
git add -A && \
git commit -m "add apps" && \
git push origin main

maj auto, possible de pull manuellement :
# flux reconcile kustomization flux-system --with-source

----[ 1.4 - Helm

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
=> évidemment ne marche pas car fs read only
-> utiliser le binaire au pif du coup

----[ 1.5 - stockage

longhorn et rook sont du stockage distribuer sous hyperconvergeance
=> besoin de plusieurs noeuds, répartissent les données d'eux mêmes, pas la main sur leur positions
n'agira pas comme un remplaçant pour le raid

=> zfs toujours dispo, inclus dans les extansions de flatcar
sanoid et syncoid sont des scripts à skip donc toujours dispo
migration entre machine facilité avec syncoid

=> voir activation du sysext pour le ZFS
=> voir ce qu'il en est du future secure boot de flatcar avec le module de kernel

------[ 1.5.1 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)
=> ne pause pas de pb pour les bdd en terme de cow comme le recodsize du fs est bien supérieurà celui qu'elles utilises

------[ 1.5.2 - dataset

# un pool est volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

------[ 1.5.3 - snapshot

sudo bash -c 'cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF
'

------[ 1.5.4 - timers

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF
'

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF
'

sudo systemctl daemon-reload
sudo systemctl enable sanoid.timer
sudo systemctl start sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

----[ 1.6 - renovate

a faire

----[ 1.7 - CloudNativePG

# les apps vont demander des bdd lorsqu'elles ne sont intégrées directmenent dans celles-ci
# pour éviter une architecure monolithique un operateur sera utile pour centralisera leur déploiement et maintenance
# installation avec l'helm chart officielle par flux
# storage dans /var/lib/rancher/k3s/storage/

----[ 1.8 - redis

# dragonfly de dispo en remplacement de redis mais ne marche pas.
# pb de hardware ?
# à l'inverse de postgres ici en monolithique comme dragonfly ne traite aucune donnée critique
# operateur de dispo mais osef du HA

=> restester dragonfly sur un autre cpu

----[ 1.9 - maintenance

------[ 1.9.1 - des pods avec flux

pour chaque image on peut créer un job de scan, exemple :
flux create image repository postgres-kc \
--image=docker.io/postgres \
--interval=60m \
--export > postgres-kc-registry.yaml

# on désigne ensuite la stratégie à utiliser pour filtrer ces tags (que les patch, ques les versions mineurs, inclure les pre-release...)
flux create image policy postgres-kc \
--image-ref=postgres-kc \
--select-semver='>=1.0.0' \
--export > postgres-kc-policy.yaml

# on créer ensuite imageupdateautomation pour mettre à jour les manifest lors de la detection des nouvelles images
flux create image update flux-system \
--interval=60m \
--git-repo-ref=flux-system \
--git-repo-path="./clusters/my-cluster" \
--checkout-branch=main \
--push-branch=main \
--author-name=fluxcdbot \
--author-email=fluxcdbot@users.noreply.github.com \
--commit-template="{{range .Changed.Changes}}{{print .OldValue}} -> {{println .NewValue}}{{end}}" \
--export > ./flux-system-automation.yaml

=> test renovate en raplecement car pb sur certain semver

------[ 1.9.2 - du cluster

https://docs.k3s.io/upgrades
# k3s intègre une custom resource definition (CRD), les plans
# apparemment monitor et maj auto
prendre les plans server et agents
remplacer version par le channel field
mettre en latest

=> test renovate, apprement peut le maintenir à jour

--[ 2 - apps

----[ 2.0 - Certificat Authority

# créer la key
openssl genrsa -out lan.key 4096
# creér le root certificate (le common name indique le domaine sous lequel on signe, ici lan)
# le root certificate est à installer sur toutes les machines qui ont besoin d'accéder aux services lan
openssl req -x509 -new -nodes -key lan.key -sha256 -days 3650 -out lan.pem -subj "/CN=lan"

----[ 2.1 - searxng

openssl genrsa -out searxng.lan.key 4096
openssl req -new -key searxng.lan.key -out searxng.lan.csr  -subj "/CN=searxng.lan"
cat << EOF > searxng.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = searxng.lan
EOF
openssl x509 -req -in searxng.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out searxng.lan.crt -days 3650 -sha256 -extfile searxng.lan.ext

secret :
sudo kubectl create secret tls searxng-secret \
  --cert=./searxng.lan.crt \
  --key=./searxng.lan.key

----[ 2.2 - ocis

# pour nouveau déploiement voir installation officielle avec helm

openssl genrsa -out owncloud.lan.key 4096
openssl req -new -key owncloud.lan.key -out owncloud.lan.csr  -subj "/CN=owncloud.lan"
cat << EOF > owncloud.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = owncloud.lan
EOF
openssl x509 -req -in owncloud.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out owncloud.lan.crt -days 3650 -sha256 -extfile owncloud.lan.ext

secret :
sudo kubectl create secret tls owncloud-secret \
  --cert=./owncloud.lan.crt \
  --key=./owncloud.lan.key

----[ 2.3 - navidrome

# pas d'oidc a cause de l'api subsonic
https://github.com/navidrome/navidrome/issues/858

openssl genrsa -out mytube.lan.key 4096
openssl req -new -key mytube.lan.key -out mytube.lan.csr  -subj "/CN=mytube.lan"
cat << EOF > mytube.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = mytube.lan
EOF
openssl x509 -req -in mytube.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out mytube.lan.crt -days 3650 -sha256 -extfile mytube.lan.ext

secret :
sudo kubectl create secret tls navidrome-secret \
  --cert=./mytube.lan.crt \
  --key=./mytube.lan.key

----[ 2.4 - homepage

# créer la clef privé du website
openssl genrsa -out homepage.lan.key 4096
# créer le csr (common name = server hostname = homepage.lan)
openssl req -new -key homepage.lan.key -out homepage.lan.csr -subj "/CN=homepage.lan"
# extension X509 V3
cat << EOF > homepage.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = homepage.lan
EOF
# le certificat final
openssl x509 -req -in homepage.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out homepage.lan.crt -days 3650 -sha256 -extfile homepage.lan.ext

secret :
sudo kubectl create secret tls homepage-secret \
  --cert=./homepage.lan.crt \
  --key=./homepage.lan.key

----[ 2.5 - paperless-ngx

# créer la clef privé du website
openssl genrsa -out paperless.lan.key 4096
# créer le csr (common name = server hostname = paperless.lan)
openssl req -new -key paperless.lan.key -out paperless.lan.csr -subj "/CN=paperless.lan"
# extension X509 V3
cat << EOF > paperless.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = paperless.lan
EOF
# le certificat final
openssl x509 -req -in paperless.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out paperless.lan.crt -days 3650 -sha256 -extfile paperless.lan.ext

secret :
sudo kubectl create secret tls paperless-secret \
  --cert=./paperless.lan.crt \
  --key=./paperless.lan.key

# créer le compte admin
python3 manage.py createsuperuser
=> une fois connecter au compte admin le connecter au compte social keycloak

=> data postgres :
https://medium.com/@camphul/cloudnative-pg-in-the-homelab-with-longhorn-b08c40b85384

créer une storage class désigniant le raid pour bonne location des pgdata
= cow et db = mort ?

----[ 2.6 - calibre-web-automated

# créer la clef privé du website
openssl genrsa -out calibre.lan.key 4096
# créer le csr (common name = server hostname = calibre.lan)
openssl req -new -key calibre.lan.key -out calibre.lan.csr -subj "/CN=calibre.lan"
# extension X509 V3
cat << EOF > calibre.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = calibre.lan
EOF
# le certificat final
openssl x509 -req -in calibre.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out calibre.lan.crt -days 3650 -sha256 -extfile calibre.lan.ext

secret :
sudo kubectl create secret tls calibre-secret \
  --cert=./calibre.lan.crt \
  --key=./calibre.lan.key

----{ 2.7 - jellyfin

=> consomme max cpu

openssl genrsa -out jellyfin.lan.key 4096
openssl req -new -key jellyfin.lan.key -out jellyfin.lan.csr  -subj "/CN=jellyfin.lan"
cat << EOF > jellyfin.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = jellyfin.lan
EOF
openssl x509 -req -in jellyfin.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out jellyfin.lan.crt -days 3650 -sha256 -extfile jellyfin.lan.ext

secret :
sudo kubectl create secret tls jellyfin-secret \
  --cert=./jellyfin.lan.crt \
  --key=./jellyfin.lan.key
