+-------------------------------------------------------------------------+
|=-------------------=[ Immutable hosting platform ]=--------------------=|
|-------------------------------------------------------------------------|
|=------------------=[ uCore, RAID ZFS, K3S, GitOps ]=-------------------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation
    1.1 - rebase uCore
    1.2 - k3s
    1.3 - Flux CD
    1.4 - Helm
    1.5 - stockage
        1.5.1 - SecureBoot avec kmod ZFS
        1.5.2 - pool mirroir
        1.5.3 - dataset
        1.5.4 - snapshot 
            1.5.4.1 - service et timer
        1.5.5 - backup
            1.5.5.1 - service et timer
    1.6 - redis
    1.7 - maintenance
        1.7.1 - du server en maj auto
        1.7.2 - des pods avec flux
        1.7.3 - du cluster
2 - apps
    2.0 - Certificat Authority
    2.1 - headlamp
    2.2 - keycloak 
    2.3 - Collabora 
    2.4 - owncloud infini scale (oCIS)
    2.5 - Searxng
    2.6 - navidrome
    2.7 - paperless-ngx
    2.8 - calibre-web automated
    2.9 - jellyfin
    2.10 - overleaf 
    2.11 - stalwart & roundcube

--[ 0 - Introduction

--[ 1 - installation

core os rebase en ucore
uCore car install k3s fonctionnel en une commande
comprend sanoid et tailscale par défaut
zfs avec secureboot

----[ 1.1 - rebase ucore

# créer un fichier butane qui comprend =  clef ssh, conf réseaux, maj et reboot auto + rebase uCore en stable-zfs
-> https://github.com/ublue-os/ucore?tab=readme-ov-file#auto-rebase-install + https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/
butane --pretty --strict .\truc.bu > .\truc.ign

wget le fichier ignition
sudo coreos-installer install /dev/sda --ignition-file /truc.ign

!! bien mettre un hostname à la machine
=> à spec dans ign
!! les noeuds sous k8S sont identifier par nom de domaine
!! si ignoré la machine se l'attribut par dns
!! risque de récupérer le nom d'un service hebergé, risque de changement de nom de domaine par la suite ce qui entrainer une fausse addition de noeud et une migration des pods

!! attention au dns, certaines app peuvent vouloir résoudre leur hostname (e.g. owncloud)

tailscale up
tailscale ip = 100.102.192.19

----[ 1.2 - k3s

curl -sfL https://get.k3s.io | sh -

# intègre traefik pour l'ingress
# traefik peut utiliser une custom resource definition (CRD), l'ingressroute

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

# doit modif les infos tls du cert pour autoriser les co depuis l'int tailscale

cat << EOF > /etc/rancher/k3s/config.yaml
tls-san: 
  - 100.102.192.19
EOF

kubectl -n kube-system delete secrets/k3s-serving
mv /var/lib/rancher/k3s/server/tls/dynamic-cert.json /tmp/dynamic-cert.json

systemctl restart k3s

----[ 1.3 - FluxCD

# heberge les manifest sous git
# flux reconcilie le cluster avec l'état attendu sous git
# monitoring des repository avec maj auto

curl -s https://fluxcd.io/install.sh | sudo bash
. <(flux completion bash)
cp  /etc/rancher/k3s/k3s.yaml ~/.kube/config
flux check --pre

# bootstrap GitHub Personal Account avec les components pour l'automation
export GITHUB_TOKEN=<your-token>
export GITHUB_USER=<your-username>
flux bootstrap github \
  --components-extra=image-reflector-controller,image-automation-controller \
  --owner=$GITHUB_USER \
  --repository=k3s \
  --branch=main \
  --path=clusters/my-cluster \
  --read-write-key \
  --personal

cloner le repo
mettre les manifest dans /clusters/my-cluster
  attention : les manifests doivent spécifier le namespace à utiliser sinon timeout
push :
git add -A && \
git commit -m "add apps" && \
git push origin main

maj auto, possible de pull manuellement :
# flux reconcile kustomization flux-system --with-source

----[ 1.4 - Helm

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
=> évidemment ne marche pas car fs read only
-> utiliser le binaire au pif du coup

----[ 1.5 - stockage

longhorn et rook sont du stockage distribuer sous hyperconvergeance
=> besoin de plusieurs noeuds, répartissent les données d'eux mêmes, pas la main sur leur positions
n'agira pas comme un remplaçant pour le raid

=> zfs toujours dispo, inclus dans les extansions de flatcar
sanoid et syncoid sont des scripts à skip donc toujours dispo
migration entre machine facilité avec syncoid

=> voir activation du sysext pour le ZFS
=> voir ce qu'il en est du future secure boot de flatcar avec le module de kernel

------[ 1.5.1 - SecureBoot avec kmod ZFS

# ZFS ne marchera pas par défaut sous secureboot avant d'avoir importer la clef publique en machine-owner key (MOK)

sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der
-> demande de saisir un mot de passe, sera demander au prochain démarrage pour enregistrer la MOK

------[ 1.5.2 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)
=> ne pause pas de pb pour les bdd en terme de cow comme le recodsize du fs est bien supérieurà celui qu'elles utilises

------[ 1.5.3 - dataset

# un pool est un volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

------[ 1.5.4 - snapshot

cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF

--------[ 1.5.4.1 - service et timer

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF

cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable sanoid.timer
systemctl start sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

------[ 1.5.5 - backup

restic vers s3
passage vers glacier si coute trop chère

céer un compte de service restic sur aws avec les bonnes stratégie
https://restic.readthedocs.io/en/latest/080_examples.html

export AWS_DEFAULT_REGION="redacted"
export RESTIC_REPOSITORY="redacted"
export AWS_ACCESS_KEY_ID="redacted"
export AWS_SECRET_ACCESS_KEY="redacted"
export RESTIC_PASSWORD="redacted"

# initie le dépot
restic init

# première backup
restic backup /mnt/raid/nas/

--------[ 1.5.5.1 - service et timer

=> à automatiser

cat << EOF > backup.sh
#!/bin/bash
export AWS_DEFAULT_REGION="redacted"
export RESTIC_REPOSITORY="redacted"
export AWS_ACCESS_KEY_ID="redacted"
export AWS_SECRET_ACCESS_KEY="redacted
export RESTIC_PASSWORD="redacted"
/home/core/restic backup /mnt/raid/nas/
/home/core/restic forget --keep-last 10
/home/core/restic prune
EOF

cat << EOF > /etc/systemd/system/restic.service

EOF

cat << EOF > /etc/systemd/system/restic.timer

EOF

----[ 1.6 - redis

# dragonfly de dispo en remplacement de redis mais ne marche pas.
# pb de hardware ?
# à l'inverse de postgres ici en monolithique comme redis ne traite aucune donnée critique

----[ 1.7 - maintenance

------[ 1.7.1 - du serveur sous maj auto

systemctl edit --force --full rpm-ostreed-upgrade-reboot.service

'
# workaround for missing reboot policy
# https://github.com/coreos/rpm-ostree/issues/2843
[Unit]
Description=rpm-ostree upgrade and reboot
ConditionPathExists=/run/ostree-booted

[Service]
Type=simple
ExecStart=/usr/bin/rpm-ostree upgrade --reboot
#StandardOutput=null
Restart=always
RestartSec=60s
'

cat << EOF > /etc/systemd/system/rpm-ostreed-upgrade-reboot.timer
[Unit]
Description=rpm-ostree upgrade and reboot trigger
ConditionPathExists=/run/ostree-booted

[Timer]
OnCalendar=*-*-* 4:00:00

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable rpm-ostreed-upgrade-reboot.timer
systemctl start rpm-ostreed-upgrade-reboot.timer

reboot

------[ 1.7.1 - des pods

images en latest
=> marche pour certaines dépôts (docker, quay)
flux image update automation pour le reste (ghrc)
=> ne marche que sur du semver, nul pour le reste

## flux image update automation
# pour chaque image on peut créer un job de scan, exemple :
flux create image repository podinfo \
--image=ghcr.io/stefanprodan/podinfo \
--interval=60m \
--export > ./podinfo-registry.yaml

# on désigne ensuite la stratégie à utiliser pour filtrer ces tags (que les patch, ques les versions mineurs, inclure les pre-release...)
flux create image policy podinfo \
--image-ref=podinfo \
--select-semver='>=1.0.0' \
--export > ./podinfo-policy.yaml

# on créer ensuite imageupdateautomation pour mettre à jour les manifest lors de la detection des nouvelles images
flux create image update flux-system \
--interval=60m \
--git-repo-ref=flux-system \
--git-repo-path="./clusters/my-cluster" \
--checkout-branch=main \
--push-branch=main \
--author-name=fluxcdbot \
--author-email=fluxcdbot@users.noreply.github.com \
--commit-template="{{range .Changed.Changes}}{{print .OldValue}} -> {{println .NewValue}}{{end}}" \
--export > ./flux-system-automation.yaml

voir rennovate si besoin
https://www.mend.io/renovate/

------[ 1.7.2 - du cluster

https://docs.k3s.io/upgrades
# k3s intègre une custom resource definition (CRD), les plans
# apparemment monitor et maj auto
prendre les plans server et agents
remplacer version par le channel field
mettre en latest

--[ 2 - apps

----[ 2.0 - Certificat Authority

# créer la key
openssl genrsa -out lan.key 4096
# creér le root certificate (le common name indique le domaine sous lequel on signe, ici lan)
# le root certificate est à installer sur toutes les machines qui ont besoin d'accéder aux services lan
openssl req -x509 -new -nodes -key lan.key -sha256 -days 3650 -out lan.pem -subj "/CN=lan"

# Pour la partie DNS ici on utilise pas un server DNS local mais NextDNS
# on fait une simple réécriture *.lan → 100.102.192.19 depuis Nextdns
# bien utiliser le DNS de tailscale avec nextdns en global nameserver, sans magicdns (obligatoire pour android)
# activer l'override des local DNS pour que toutes les requêtes DNS passe par tailscale (system-wide DNS sur tout les appareils)

----[ 2.1 - Headlamp

# solution pour superviser le cluster k3s, consulter les logs des pod, entrer dans des shell...

# créer le compte de service
https://headlamp.dev/docs/latest/installation/#create-a-service-account-token

openssl genrsa -out homepage.lan.key 4096

openssl req -new -key homepage.lan.key -out homepage.lan.csr -subj "/CN=homepage.lan"

cat << EOF > homepage.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = homepage.lan
EOF

openssl x509 -req -in homepage.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out homepage.lan.crt -days 3650 -sha256 -extfile homepage.lan.ext

sudo kubectl create secret tls homepage-secret \
  --cert=./homepage.lan.crt \
  --key=./homepage.lan.key

----[ 2.2 - keycloak

# Solution de SSO pour centraliser la gestion des identités et gestion d'accès, permet la customisation des flux d'authentification et d'utiliser des méthodes modernes (passwordless
# pour la mise en place du passwordless besoin d'un server mail pour envoyer le lien d'enregistrement à l'utilisateur (voir 2.11)

realm lan
créer les clients :
ocis web : ocis
desktop : xdXOt13JKxym1B1QcEncf2XDkLAexMBFwiT9j6EfhhHFJhs2KM9jbjTmf8JBXE69
android : e4rAsNUSIUs0lF4nbv9FmCeUkTlV9GdgTLDH1b5uie7syb90SzEVrbN7HIpmWJeD
paperless web : paperless

Access settings :
Root URL : https://owncloud.lan/*

Redirect URIs :
web : https://owncloud.lan/*
Desktop : http://127.0.0.1
Android : oc://android.owncloud.com

Web origins : *

Capability config :
toutes les Authentication flow cochables

ssl:
----
openssl genrsa -out keycloak.lan.key 4096
openssl req -new -key keycloak.lan.key -out keycloak.lan.csr  -subj "/CN=keycloak.lan"
cat << EOF > keycloak.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = keycloak.lan
EOF
openssl x509 -req -in keycloak.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out keycloak.lan.crt -days 3650 -sha256 -extfile keycloak.lan.ext

secret :
kubectl create secret tls keycloak-secret \
  --cert=./keycloak.lan.crt \
  --key=./keycloak.lan.key

----[ 2.3 - Collabora

# suite office en ligne, s'utilise depuis ocis par WOPI

openssl genrsa -out office.lan.key 4096
openssl req -new -key office.lan.key -out office.lan.csr  -subj "/CN=office.lan"
cat << EOF > office.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = office.lan
EOF
openssl x509 -req -in office.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out office.lan.crt -days 3650 -sha256 -extfile office.lan.ext

secret :
kubectl create secret tls office-secret \
  --cert=./office.lan.crt \
  --key=./office.lan.key

----[ 2.4 - ocis

# owncloud version moderne, légère, écrite en go

besoin d'une deuxième instance pour la deuxième instance et de plusieurs services :
wopi -> service entre instance de collab et collabora
registry -> com entre services, instance de collab et instance principale

Besoin d'un Content Security Policy (CSP) dans ocis-config pour autoriser Collabora et keycloak
csp.yaml :
---------
cat << EOF > csp.yaml
directives:
  child-src:
    - '''self'''
  connect-src:
    - '''self'''
    - 'blob:'
    - 'https://keycloak.lan'
  default-src:
    - '''none'''
  font-src:
    - '''self'''
  frame-ancestors:
    - '''self'''
  frame-src:
    - '''self'''
    - 'blob:'
    - 'https://embed.diagrams.net/'
    # In contrary to bash and docker the default is given after the | character
    - 'https://office.lan'
    # This is needed for the external-sites web extension when embedding sites
    - 'https://docs.opencloud.eu'
  img-src:
    - '''self'''
    - 'data:'
    - 'blob:'
    - 'https://raw.githubusercontent.com/opencloud-eu/awesome-apps/'
    # In contrary to bash and docker the default is given after the | character
    - 'https://office.lan'
  manifest-src:
    - '''self'''
  media-src:
    - '''self'''
  object-src:
    - '''self'''
    - 'blob:'
  script-src:
    - '''self'''
    - '''unsafe-inline'''
  style-src:
    - '''self'''
    - '''unsafe-inline'''
EOF

ssl
---
openssl genrsa -out owncloud.lan.key 4096
openssl req -new -key owncloud.lan.key -out owncloud.lan.csr  -subj "/CN=owncloud.lan"
cat << EOF > owncloud.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = owncloud.lan
EOF
openssl x509 -req -in owncloud.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out owncloud.lan.crt -days 3650 -sha256 -extfile owncloud.lan.ext

secret :
kubectl create secret tls owncloud-secret \
  --cert=./owncloud.lan.crt \
  --key=./owncloud.lan.key

openssl genrsa -out wopi.lan.key 4096
openssl req -new -key wopi.lan.key -out wopi.lan.csr  -subj "/CN=wopi.lan"
cat << EOF > wopi.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = wopi.lan
EOF
openssl x509 -req -in wopi.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out wopi.lan.crt -days 3650 -sha256 -extfile wopi.lan.ext

secret :
kubectl create secret tls wopi-secret \
  --cert=./wopi.lan.crt \
  --key=./wopi.lan.key

----[ 2.5 - searxng

# metamoteur de recherche pour des resultats plus opti

# créer la clef privé du website
openssl genrsa -out searxng.lan.key 4096

# créer le csr (common name = server hostname = homepage.lan)
openssl req -new -key searxng.lan.key -out searxng.lan.csr  -subj "/CN=searxng.lan"

# extension X509 V3
cat << EOF > searxng.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = searxng.lan
EOF

# le certificat final
openssl x509 -req -in searxng.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out searxng.lan.crt -days 3650 -sha256 -extfile searxng.lan.ext

# le secret :
sudo kubectl create secret tls searxng-secret \
  --cert=./searxng.lan.crt \
  --key=./searxng.lan.key

----[ 2.6 - navidrome

# serveur de musique avec l'API subsonic

openssl genrsa -out mytube.lan.key 4096

openssl req -new -key mytube.lan.key -out mytube.lan.csr  -subj "/CN=mytube.lan"

cat << EOF > mytube.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = mytube.lan
EOF

openssl x509 -req -in mytube.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out mytube.lan.crt -days 3650 -sha256 -extfile mytube.lan.ext

sudo kubectl create secret tls navidrome-secret \
  --cert=./mytube.lan.crt \
  --key=./mytube.lan.key

----[ 2.7 - paperless-ngx

# Solution d'archivage de documents

openssl genrsa -out paperless.lan.key 4096

openssl req -new -key paperless.lan.key -out paperless.lan.csr -subj "/CN=paperless.lan"

cat << EOF > paperless.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = paperless.lan
EOF

openssl x509 -req -in paperless.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out paperless.lan.crt -days 3650 -sha256 -extfile paperless.lan.ext

sudo kubectl create secret tls paperless-secret \
  --cert=./paperless.lan.crt \
  --key=./paperless.lan.key

# créer le compte admin
python3 manage.py createsuperuser

----[ 2.8 - calibre-web-automated

# bibliothèque en ligne

openssl genrsa -out calibre.lan.key 4096

openssl req -new -key calibre.lan.key -out calibre.lan.csr -subj "/CN=calibre.lan"

cat << EOF > calibre.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = calibre.lan
EOF

openssl x509 -req -in calibre.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out calibre.lan.crt -days 3650 -sha256 -extfile calibre.lan.ext

sudo kubectl create secret tls calibre-secret \
  --cert=./calibre.lan.crt \
  --key=./calibre.lan.key

----{ 2.9 - jellyfin

# serveur multimédia

openssl genrsa -out jellyfin.lan.key 4096

openssl req -new -key jellyfin.lan.key -out jellyfin.lan.csr  -subj "/CN=jellyfin.lan"

cat << EOF > jellyfin.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = jellyfin.lan
EOF

openssl x509 -req -in jellyfin.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out jellyfin.lan.crt -days 3650 -sha256 -extfile jellyfin.lan.ext

sudo kubectl create secret tls jellyfin-secret \
  --cert=./jellyfin.lan.crt \
  --key=./jellyfin.lan.key

----[ 2.10 - overleaf

# editeur LaTex en ligne

openssl genrsa -out tex.lan.key 4096
openssl req -new -key tex.lan.key -out tex.lan.csr  -subj "/CN=tex.lan"
cat << EOF > tex.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = tex.lan
EOF
openssl x509 -req -in tex.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out tex.lan.crt -days 3650 -sha256 -extfile tex.lan.ext

secret :
kubectl create secret tls tex-secret \
  --cert=./tex.lan.crt \
  --key=./tex.lan.key

----[ 2.11 - stalwart & roundcube

# pour la com par mail des app (e.g. les credential reset de keycloak)

roundcube pour l'instant car
pas de webclient intégré à stalwart atm
pas de webclient intégré à ocis atm

config dns :
------------
hostname : smtp (identique au service)
domaine : mail.lan
comptes en user@mail.lan

openssl genrsa -out mail.lan.key 4096
openssl req -new -key mail.lan.key -out mail.lan.csr  -subj "/CN=mail.lan"
cat << EOF > mail.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = mail.lan
EOF
openssl x509 -req -in mail.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out mail.lan.crt -days 3650 -sha256 -extfile mail.lan.ext

secret :
kubectl create secret tls mail-secret \
  --cert=./mail.lan.crt \
  --key=./mail.lan.key

openssl genrsa -out webmail.lan.key 4096
openssl req -new -key webmail.lan.key -out webmail.lan.csr  -subj "/CN=webmail.lan"
cat << EOF > webmail.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = webmail.lan
EOF
openssl x509 -req -in webmail.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out webmail.lan.crt -days 3650 -sha256 -extfile webmail.lan.ext

secret :
kubectl create secret tls webmail-secret \
  --cert=./webmail.lan.crt \
  --key=./webmail.lan.key

----------------- notes

logs :
sudo kubectl logs pod-name

shell :
sudo kubectl exec --stdin --tty pod -- /bin/sh

# toutes les ressources
kubectl get 

# voir le détails des tag d'images observé
kubectl -n flux-system describe imagerepositories searxng

# voir le status du traking des images
flux get images all --all-namespaces

# debug les reconciliation
flux get ks

flux reconcile kustomization flux-system --with-source

=> pb de node unschedulable
kubectl patch nodes searxng.lan --patch '{"spec":{"unschedulable": false}}'

au niveau du dns:
les container on connaissances des services côté non de domaine (ex : "redis://redis:6379" renvoi vers le service redis)
pour modifier le fichier host, notamment pour une connaissance d'un domaine hors k3s on peut uitliser hostaliases
exemple : 
    spec:
      hostAliases:
      - ip: "192.168.1.200"
        hostnames:
          - "owncloud.lan"
          - "office.lan"
