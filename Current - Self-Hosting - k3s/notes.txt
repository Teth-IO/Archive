+-------------------------------------------------------------------------+
|=-------------------=[ Immutable hosting platform ]=--------------------=|
|-------------------------------------------------------------------------|
|=------------------=[ uCore, RAID ZFS, K3S, GitOps ]=-------------------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation
    1.1 - rebase uCore
    1.2 - k3s
    1.3 - stockage
        1.3.1 - SecureBoot avec kmod ZFS
        1.3.2 - pool mirroir
        1.3.3 - dataset
		1.3.4 - Gestion du stockage sous kubernetes
	1.4 - Certificat Authority
	1.5 - DNS
	1.6 - headlamp
	1.7 - Gitea
    1.8 - Flux CD
	1.9 - SOPS
	1.10 - cert-manager
    1.11 - maintenance
        1.11.1 - du server en maj auto
        1.11.2 - des pods avec renovate
        1.11.3 - du cluster
		1.11.4 - du stockage
			1.11.4.1 - snapshot 
            	1.11.4.1.1 - service et timer
        	1.11.4.2 - backup
            	1.11.4.2.1 - script
2 - apps
    2.1 - keycloak 
    2.2 - Collabora 
    2.3 - owncloud infini scale (oCIS)
    2.4 - Searxng
    2.5 - navidrome
    2.6 - paperless-ngx
    2.7 - calibre-web automated
    2.8 - jellyfin
    2.9 - Trilium Next
	2.10 - ntfy
	2.11 - tailscale operator
	2.12 - technitium

--[ 0 - Introduction

--[ 1 - installation

core os rebase en ucore
uCore car install k3s fonctionnel en une commande
comprend sanoid et tailscale par défaut
zfs avec secureboot

----[ 1.1 - rebase ucore

# créer un fichier butane qui comprend =  clef ssh, conf réseaux, maj et reboot auto + rebase uCore stable
-> https://github.com/ublue-os/ucore?tab=readme-ov-file#auto-rebase-install + https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/
butane --pretty --strict .\truc.bu > .\truc.ign

wget le fichier ignition
sudo coreos-installer install /dev/sda --ignition-file /truc.ign

!! bien mettre un hostname à la machine
=> à spec dans ign
!! les noeuds sous k8S sont identifier par nom de domaine
!! si ignoré la machine se l'attribut par dns
!! risque de récupérer le nom d'un service hebergé, risque de changement de nom de domaine par la suite ce qui entrainer une fausse addition de noeud et une migration des pods

!! attention au dns, certaines app peuvent vouloir résoudre leur hostname (e.g. owncloud)

tailscale up
tailscale ip = 100.102.192.19

----[ 1.2 - k3s

curl -sfL https://get.k3s.io | sh -

# intègre traefik pour l'ingress
# traefik peut utiliser une custom resource definition (CRD), l'ingressroute

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

# doit modif les infos tls du cert pour autoriser les co depuis l'int tailscale

cat << EOF > /etc/rancher/k3s/config.yaml
tls-san: 
  - 100.102.192.19
EOF

kubectl -n kube-system delete secrets/k3s-serving
mv /var/lib/rancher/k3s/server/tls/dynamic-cert.json /tmp/dynamic-cert.json

systemctl restart k3s

----[ 1.3 - stockage

longhorn et rook sont du stockage distribuer sous hyperconvergeance
=> besoin de plusieurs noeuds, répartissent les données d'eux mêmes, pas la main sur leur positions
n'agira pas comme un remplaçant pour le raid

zfs dispo avec secureboot
sanoid dispo pour automatiser la prise de snapshot
récupérer le binaire de restic pour les backup

------[ 1.3.1 - SecureBoot avec kmod ZFS

# ZFS ne marchera pas par défaut sous secureboot avant d'avoir importer la clef publique en machine-owner key (MOK)

sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der
-> demande de saisir un mot de passe, sera demander au prochain démarrage pour enregistrer la MOK

------[ 1.3.2 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)
=> ne pause pas de pb pour les bdd en terme de cow comme le recodsize du fs est bien supérieurà celui qu'elles utilises

------[ 1.3.3 - dataset

# un pool est un volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

------[ 1.3.4 - Gestion du stockage sous kubernetes

on est ici sous un cluster d'un noeud donc pas de volume distribuer
par défaut kubernetes marche avec des volumes
ici par simplicité on preferera des volumes en localPath qui pointe vers le raid
les helm vont créer des pvc qui avec local-path-provisioner de k3s irront par défaut sous /var/lib/rancher/k3s/storage
on peut modifier le chemin dans la ConfigMap local-path-config

----[ 1.4 - Certificat Authority

# créer la key
openssl genrsa -out lan.key 4096
# creér le root certificate (le common name indique le domaine sous lequel on signe, ici lan)
# le root certificate (lan.pem) est à installer sur toutes les machines qui ont besoin d'accéder aux services lan
openssl req -x509 -new -nodes -key lan.key -sha256 -days 3650 -out lan.pem -subj "/CN=lan"

# Pour la partie DNS ici on utilise pas un server DNS local mais NextDNS
# on fait une simple réécriture *.lan → 100.102.192.19 depuis Nextdns
# bien utiliser le DNS de tailscale avec nextdns en global nameserver, sans magicdns (obligatoire pour android)
# activer l'override des local DNS pour que toutes les requêtes DNS passe par tailscale (system-wide DNS sur tout les appareils)

# Pour chaque app création d'un certif signé par lan.
# Exemple pour gitea :

# créer la clef privé du website
openssl genrsa -out gitea.lan.key 4096

# créer le csr (common name = server hostname = gitea.lan)
openssl req -new -key gitea.lan.key -out gitea.lan.csr  -subj "/CN=gitea.lan"

# extension X509 V3
cat << EOF > gitea.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = gitea.lan
EOF

# le certificat final
openssl x509 -req -in gitea.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out gitea.lan.crt -days 3650 -sha256 -extfile gitea.lan.ext

# enregistrement de la pair en secret dans kubernetes pour application au niveau de l'ingress
kubectl create secret tls gitea-secret \
  --cert=./gitea.lan.crt \
  --key=./gitea.lan.key

Le secret est ensuite désigner dans l'IngressRoute pour  assurer la terminaison TLS
Besoin de créer et fournir manuellement des certificats pour Gitea et headlamp, la création et gestion des certificats est automatisé par la suite avec cert-manager (1.10) installé par healmRealease (FluxCD)
cert-manager ne marche que sur des Ingress et Gateway

----[ 1.5 - DNS

Le server est boostrap en 9.9.9.9 et celà est hérité par k3s (coredns)
Les noms de domaine interne en lan (1.4) sont gérer déclaré au besoin par fichier host pour le serveur, configMap pour coredns et hostAliases pour les pods à défaut d'un server DNS externe.
Un pod technitium (2.13) est le server DNS pour la zone lan au sein d'un réseau tailscale qui englobe toute l'infra ce qui la rend agnostic lan/wan
les clients accèdes aux services à travers l'ip du server sous tailscale, technitium est le global nameserver du tailnet avec l'override DNS server pour l'utiliser dans la résolution des noms hors du tailnet et donc s'épargner la configuration du server dns de chaque machine.
le server doit être exempt de cet override dns de tailscale sinon lui même et les conteneurs n'auront plus de DNS en cas de redémarrage :
tailscale set --accept-dns=false

Tout doit pointer à l'ip server pour arriver à l'ingress qui fait l'offload TLS et le routage, les noms doivent donc correspondre au match SNI de l'ingressroute de traefik pour être bien routé et au domaine indiquer dans l'enregistrement des cert TLS pour la vérification du navigateur

----[ 1.6 - Headlamp

# solution pour superviser le cluster k3s, consulter les logs des pod, entrer dans des shell...

# créer le compte de service
https://headlamp.dev/docs/latest/installation/#create-a-service-account-token

----[ 1.7 - Gitea

# version self hosté de github
# en attendant fluxcd on applique un manifest manuellement
kubectl apply -f gitea.yaml

## intégrer openid (après keycloak en 2.1) :
depuis l'interface web aller sur administration du site > identié et accès > source d'authentification
créer une entrée Oauth2 avec un fournisseur OpenID connect

côté keycloak mettre client authentification pour credential

création d'un secret avec le pem :
kubectl create secret generic lan-pem --from-file=lan.pem
=> le mapper dans le conteneur en lecture seule

openid connect url de découverte :
https://keycloak.lan/realms/lan/.well-known/openid-configuration 

----[ 1.8 - FluxCD

# heberge les manifest sous git
# flux reconcilie le cluster avec l'état attendu sous git
# monitoring des repository avec maj auto

curl -s https://fluxcd.io/install.sh | sudo bash
. <(flux completion bash)
cp  /etc/rancher/k3s/k3s.yaml ~/.kube/config
flux check --pre

## bootstrap vers Gitea

# besoin que le hostname gitea.lan soit connu de CoreDNS (ce qui pose problème en l'absence de server DNS)
# on peut appliquer un ConfigMap pour le définir

kind: ConfigMap
apiVersion: v1
metadata:
  name: coredns-custom
  namespace: kube-system
data:
  gitea.server: |
    gitea.lan {
      hosts {
          192.168.1.200 gitea.lan
          fallthrough
      }
    }

# bootstrap :
flux bootstrap gitea --hostname=gitea.lan --owner=admin --repository=NixOS-k3s --token-auth --private=false --personal=true --branch=main --path=k3s --ca-file=/mnt/raid/nas/ssl/lan.pem

----[ 1.9 - SOPS

# pour la gestion des secrets, permet le chiffrements de fichier et leur déchiffrements ici avec age

création de la pair de clef :
age-keygen -o keys.txt

le fichier contient la clef public et privé
la public chiffre et la privé déchiffre
si pas spécifier dans la commande pour déchiffrement le programme la cherche dans ~/.config/sops/age/keys.txt

enrollement dans le cluster :
cat keys.txt |
kubectl create secret generic sops-age \
--namespace=flux-system \
--from-file=keys.txt=/dev/stdin

création d'un fichier de config SOPS à la racine du dépôt pour la commande sops soit preconfigurer avec les bons champ :
.sops.yaml
creation_rules:
  - encrypted_regex: '^(data|stringData)$'
    age: age1dyl0es8xaqda3qr0dmlrapdgz5ffslkv2sag5amccus8qdfgtsqslmk4hy

pour pour configurer le déchiffrement dans le cluster on peut éditer directement le kustomization sous gotk-sync.yaml avec decryption.provider sops et decryption.secretRef.name sops-age

pour le chiffrement il suffit de faire : sops encrypt -i *.yaml
chiffre in-place (remplace le fichier visé par sa version chiffré)
la clef publique pour le chiffrement est récupéré du fichier de config (le fichier doit être dans le chemin d'où la commande est lancé)

hors cluster les secrets peuvent être passer à d'autre programme, voir https://getsops.io/docs/#passing-secrets-to-other-processes (utilisé dans le script de backup 1.11.4.2.1)

----[ 1.10 - cert-manager

# notre root CA est déclaré dans un secret et est utilisé en tant qu'issuer

----[ 1.11 - maintenance

------[ 1.11.1 - du serveur sous maj auto

systemctl edit --force --full rpm-ostreed-upgrade-reboot.service

'
# workaround for missing reboot policy
# https://github.com/coreos/rpm-ostree/issues/2843
[Unit]
Description=rpm-ostree upgrade and reboot
ConditionPathExists=/run/ostree-booted

[Service]
Type=simple
ExecStart=/usr/bin/rpm-ostree upgrade --reboot
#StandardOutput=null
Restart=always
RestartSec=60s
'

cat << EOF > /etc/systemd/system/rpm-ostreed-upgrade-reboot.timer
[Unit]
Description=rpm-ostree upgrade and reboot trigger
ConditionPathExists=/run/ostree-booted

[Timer]
OnCalendar=*-*-* 4:00:00

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable rpm-ostreed-upgrade-reboot.timer
systemctl start rpm-ostreed-upgrade-reboot.timer

reboot

------[ 1.11.2 - des pods

## Renovate
# setup les gitea actions :
-> gitea runner :

créer un runner dépuis l'interface web, récupérer le token pour le manifest du runner
besoin de l'image Docker in Docker (dind) comme on est sur kubernetes
utilisation de l'image de vegardit https://github.com/vegardit/docker-gitea-act-runner sinon pb de timeout réseau ce qui fait échoué les actions
nécessite securityContext: privileged: true
comme ici le CA et les nom de domaine ne sont pas resolvable sans configuration on tout nouveau monde de douleur s'ouvre à nous pour que les conteneurs dans le pod puisse se connecter :
	-> besoin que le reseau docker soit sur host sinon pas de résolution dns possible GITEA_RUNNER_JOB_CONTAINER_NETWORK
	-> besoin de bind mount le pem venant du volume mount du pod pour fournir le certificat

=> vérifier que le runner apparait bien dans l'interface web

# rennovate

suivre :
https://about.gitea.com/resources/tutorials/use-gitea-and-renovate-bot-to-automatically-monitor-software-packages

créer un compte pour le bot renovate + un token
creér un renovate.json à la racine du dépôt sur lequel renovate doit agir + ajouter le comppte du bot en collaborateur
créer un dépôts pour la config de renovate
mettre dans ce dépôt le config.json et le workflows de renovate
mettre le token dans ce dépôt

# pb aléatoire du runner

git semble ignorer le system store alors que le pem est bien mapper
=> ajout d'env var supplémentaire NODE_EXTRA_CA_CERTS et GIT_SSL_CAINFO pointant vers le pem, dans le workflow et le manifest

# A aussi besoin d'un token GITHUB pour récupérer des changelogs

ajout d'une env var RENOVATE_GITHUB_COM_TOKEN et ajouter le token dans le dépôt

# renovate ne lit pas les manifest déclarant plusieurs éléments (séparé par trois -)
Il ne lit que le premier

------[ 1.11.3 - du cluster

https://docs.k3s.io/upgrades
# k3s intègre une custom resource definition (CRD), les plans
# apparemment monitor et maj auto
prendre les plans server et agents
remplacer version par le channel field
mettre en latest

------[ 1.11.4 - du stockage
--------[ 1.11.4.1 - snapshot du dataset

avec sanoid

cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF

----------[ 1.11.4.1.1 - service et timer

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF

cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable sanoid.timer
systemctl start sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

--------[ 1.11.4.2 - backup du raid

restic vers amazon s3

céer un compte de service restic sur aws avec les bonnes stratégie
https://restic.readthedocs.io/en/latest/080_examples.html

besoin d'une pair de variables 

AWS_DEFAULT_REGION="redacted"
RESTIC_REPOSITORY="redacted"
AWS_ACCESS_KEY_ID="redacted"
AWS_SECRET_ACCESS_KEY="redacted"
RESTIC_PASSWORD="redacted"

# initie le dépot
restic init

# première backup
restic backup /mnt/raid/nas/

----------[ 1.11.4.2.1 - script

afin de stocker les variables précédentes de façon sécurisé on met les variables précédentes dans un env.json

{
        "AWS_DEFAULT_REGION": "redacted",
        "RESTIC_REPOSITORY": "redacted",
        "AWS_ACCESS_KEY_ID": "redacted",
        "AWS_SECRET_ACCESS_KEY": "redacted",
        "RESTIC_PASSWORD": "redacted"
}

que l'on vient chiffrer avec notre clef publique age et SOPS

sops encrypt -i test.json

une commande #sops decrypt env.json les déchiffres biens en stdout
avec le process exec-env on peut déchiffrer et export les variables directement dans un programme, ici sh

sops exec-env env.json 'sh'

donc pour un script qui lance la backup (avec # curl l'ouput des commande vers ntfy (2.11)) :

launch.sh
#!/bin/bash
/home/core/sops exec-env /home/core/env.json 'sh /home/core/backup.sh'

backup.sh
#!/bin/bash
curl -X POST -d "$(/home/core/restic backup /mnt/raid/nas/ --exclude /mnt/raid/nas/jellyfin/data)" https://ntfy.lan/Backup
curl -X POST -d "$(/home/core/restic forget --keep-last 10)" https://ntfy.lan/Backup
curl -X POST -d "$(/home/core/restic prune)" https://ntfy.lan/Backup
exit

# curl l'ouput des commande vers ntfy (2.11) pour travail en headless

--[ 2 - apps

----[ 2.1 - keycloak

# Solution de SSO pour centraliser la gestion des identités et gestion d'accès, permet la customisation des flux d'authentification et d'utiliser des méthodes modernes (passwordless)

realm lan
créer les clients :
ocis web : ocis
desktop : xdXOt13JKxym1B1QcEncf2XDkLAexMBFwiT9j6EfhhHFJhs2KM9jbjTmf8JBXE69
android : e4rAsNUSIUs0lF4nbv9FmCeUkTlV9GdgTLDH1b5uie7syb90SzEVrbN7HIpmWJeD
paperless web : paperless

Access settings :
Root URL : https://owncloud.lan/*

Redirect URIs :
web : https://owncloud.lan/*
Desktop : http://127.0.0.1
Android : oc://android.owncloud.com

Web origins : *

Capability config :
toutes les Authentication flow cochables

# pour enregister les passkeys se co avec le compte user lan https://keycloak.lan/realms/lan/account/

# besoin de modifier dans l'authentification du realm l'entrée broswer (cookie -> idp redirector -> webauthn passwordless authenticator) pour prioriser le passwordless
revoir le bug du account-console qui était innaccessible (droit du compte, impersonation du compte admin realms master, bidouillage de l'entré account et account-console...)

# Pour utiliser des passkeys sous android besoin d'un paramétarge spécifique :
l'enregistrement d'une passkey ne marche pas sur firefox android (l'enregistrer depuis un chromium)
sur chrome avec les paramètres Authentification/policies/Webauthn Passwordless Policy:
Authenticator Attachment -> si platform = échec donc laissé en not specified
Require discoverable credential -> yes
    empêche l'enregistrement en locale de la passkey (cet appareil), mais celui ci ne marche pas (google password manager dit qu'aucune clef n'est dispo car justement pas discoverable...)

donc besoin d'une app tierse pour l'enregistrer :
-> bientôt keepassdx compatible
https://github.com/Kunzisoft/KeePassDX/issues/1421
-> proton pass en attendant ou google password manager

----[ 2.2 - Collabora

# suite office en ligne, s'utilise depuis ocis par WOPI

----[ 2.3 - ocis

# owncloud version moderne, légère, écrite en go

besoin d'une deuxième instance pour le wopiu et de plusieurs services :
wopi -> service entre instance de collab et collabora
registry -> com entre services, instance de collab et instance principale

Besoin d'un Content Security Policy (CSP) dans ocis-config pour autoriser Collabora et keycloak
csp.yaml :
---------
cat << EOF > csp.yaml
directives:
  child-src:
    - '''self'''
  connect-src:
    - '''self'''
    - 'blob:'
    - 'https://keycloak.lan'
  default-src:
    - '''none'''
  font-src:
    - '''self'''
  frame-ancestors:
    - '''self'''
  frame-src:
    - '''self'''
    - 'blob:'
    - 'https://embed.diagrams.net/'
    # In contrary to bash and docker the default is given after the | character
    - 'https://collabora.lan'
    # This is needed for the external-sites web extension when embedding sites
    - 'https://docs.opencloud.eu'
  img-src:
    - '''self'''
    - 'data:'
    - 'blob:'
    - 'https://raw.githubusercontent.com/opencloud-eu/awesome-apps/'
    # In contrary to bash and docker the default is given after the | character
    - 'https://collabora.lan'
  manifest-src:
    - '''self'''
  media-src:
    - '''self'''
  object-src:
    - '''self'''
    - 'blob:'
  script-src:
    - '''self'''
    - '''unsafe-inline'''
  style-src:
    - '''self'''
    - '''unsafe-inline'''
EOF

ici 2 créations de cert ssl comme 2 instances
---------------------------------------------
openssl genrsa -out owncloud.lan.key 4096
openssl req -new -key owncloud.lan.key -out owncloud.lan.csr  -subj "/CN=owncloud.lan"
cat << EOF > owncloud.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = owncloud.lan
EOF
openssl x509 -req -in owncloud.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out owncloud.lan.crt -days 3650 -sha256 -extfile owncloud.lan.ext

secret :
kubectl create secret tls owncloud-secret \
  --cert=./owncloud.lan.crt \
  --key=./owncloud.lan.key

openssl genrsa -out wopi.lan.key 4096
openssl req -new -key wopi.lan.key -out wopi.lan.csr  -subj "/CN=wopi.lan"
cat << EOF > wopi.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = wopi.lan
EOF
openssl x509 -req -in wopi.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out wopi.lan.crt -days 3650 -sha256 -extfile wopi.lan.ext

secret :
kubectl create secret tls wopi-secret \
  --cert=./wopi.lan.crt \
  --key=./wopi.lan.key

----[ 2.4 - searxng

# metamoteur de recherche pour des resultats plus opti

----[ 2.5 - navidrome

# serveur de musique avec l'API subsonic

----[ 2.6 - paperless-ngx

# Solution d'archivage de documents

# créer le compte admin
python3 manage.py createsuperuser

----[ 2.7 - calibre-web-automated

# bibliothèque en ligne

----{ 2.8 - jellyfin

# serveur multimédia

----[ 2.9 - Trilium Next

# Application de prise de note, Gestion des connaissances dispo par interface web

----[ 2.10 - ntfy

# pub-sub notification service pour le monitoring du ZFS et du SMART des SSD
ZFS besoin de paramétrer /etc/zfs/zed.d/zed.rc :
url ntfy : https://ntfy.lan
sujet : ZFS
verbosité : 1
-> notif envoyé à https://ntfy.lan/ZFS

----[ 2.11 - tailscale operator

installation par helm car manifest chargé (+ de 5200 lignes)

permet plein d'options de réseaux utile à travers le tailnet :
    API server proxy
    Egress
    Ingress
    Cross cluster
    Cloud services
    Subnet routers and exit nodes
    App connector
    Recorder nodes
    Session recording
    Operator resource customization
    Troubleshooting
    Multi-cluster ArgoCD

----[ 2.12 - technitium

# blocking list : Hagezi multi pro ++
# upstream DOH (quad9) pour l'ECH 
# caching

le service technitium est exposer depuis tailscale pour fournir directement les port 53 udp/tcp sans passer par l'ingress qui elle est plus faite pour du TLS offloading et routage

technitium est le global nameserver sous tailscale avec l'override des serveur DNS pour l'appliquer à toutes les machines du tailnet

pour les enregistremment DNS dans technitium on créer d'abord une zone primaire pour lan. Les enregistrement NS et SOA sont créer par défaut, il y a juste à ajouter des enregistrement A pour chaque nom sous lan pointant vers l'ip du server sous tailscale

----------------- notes

Vérif de l'image utilisé par l'OS : 
rpm-ostree status

logs :
sudo kubectl logs pod-name

shell :
sudo kubectl exec --stdin --tty pod -- /bin/sh

toutes les ressources :
kubectl get 

debug les reconciliation :
flux get ks

flux reconcile kustomization flux-system --with-source

=> pb de node unschedulable
kubectl patch nodes searxng.lan --patch '{"spec":{"unschedulable": false}}'

au niveau du dns:
les container on connaissances des services côté non de domaine (ex : "redis://redis:6379" renvoi vers le service redis)
pour modifier le fichier host, notamment pour une connaissance d'un domaine hors k3s on peut uitliser hostaliases
exemple : 
    spec:
      hostAliases:
      - ip: "192.168.1.200"
        hostnames:
          - "owncloud.lan"
          - "office.lan"
