+-------------------------------------------------------------------------+
|=-------------------=[ Immutable hosting platform ]=--------------------=|
|-------------------------------------------------------------------------|
|=------------------=[ uCore, RAID ZFS, K3S, GitOps ]=-------------------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation
    1.1 - rebase uCore
    1.2 - k3s
    1.3 - stockage
        1.3.1 - SecureBoot avec kmod ZFS
        1.3.2 - pool mirroir
        1.3.3 - dataset
        1.3.4 - snapshot 
            1.3.4.1 - service et timer
        1.3.5 - backup
            1.3.5.1 - script
	1.4 - Certificat Authority
	1.5 - Gitea
    1.6 - Flux CD
    1.7 - maintenance
        1.7.1 - du server en maj auto
        1.7.2 - des pods avec flux
        1.7.3 - du cluster
	1.8 - SOPS
2 - apps
    2.1 - headlamp
    2.2 - keycloak 
    2.3 - Collabora 
    2.4 - owncloud infini scale (oCIS)
    2.5 - Searxng
    2.6 - navidrome
    2.7 - paperless-ngx
    2.8 - calibre-web automated
    2.9 - jellyfin
    2.10 - overleaf 
    2.11 - Trilium Next
	2.12 - ntfy

--[ 0 - Introduction

--[ 1 - installation

core os rebase en ucore
uCore car install k3s fonctionnel en une commande
comprend sanoid et tailscale par défaut
zfs avec secureboot

----[ 1.1 - rebase ucore

# créer un fichier butane qui comprend =  clef ssh, conf réseaux, maj et reboot auto + rebase uCore stable
-> https://github.com/ublue-os/ucore?tab=readme-ov-file#auto-rebase-install + https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/
butane --pretty --strict .\truc.bu > .\truc.ign

wget le fichier ignition
sudo coreos-installer install /dev/sda --ignition-file /truc.ign

!! bien mettre un hostname à la machine
=> à spec dans ign
!! les noeuds sous k8S sont identifier par nom de domaine
!! si ignoré la machine se l'attribut par dns
!! risque de récupérer le nom d'un service hebergé, risque de changement de nom de domaine par la suite ce qui entrainer une fausse addition de noeud et une migration des pods

!! attention au dns, certaines app peuvent vouloir résoudre leur hostname (e.g. owncloud)

tailscale up
tailscale ip = 100.102.192.19

----[ 1.2 - k3s

curl -sfL https://get.k3s.io | sh -

# intègre traefik pour l'ingress
# traefik peut utiliser une custom resource definition (CRD), l'ingressroute

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

# doit modif les infos tls du cert pour autoriser les co depuis l'int tailscale

cat << EOF > /etc/rancher/k3s/config.yaml
tls-san: 
  - 100.102.192.19
EOF

kubectl -n kube-system delete secrets/k3s-serving
mv /var/lib/rancher/k3s/server/tls/dynamic-cert.json /tmp/dynamic-cert.json

systemctl restart k3s

----[ 1.3 - stockage

longhorn et rook sont du stockage distribuer sous hyperconvergeance
=> besoin de plusieurs noeuds, répartissent les données d'eux mêmes, pas la main sur leur positions
n'agira pas comme un remplaçant pour le raid

zfs dispo avec secureboot
sanoid dispo pour automatiser la prise de snapshot
récupérer le binaire de restic pour les backup

------[ 1.3.1 - SecureBoot avec kmod ZFS

# ZFS ne marchera pas par défaut sous secureboot avant d'avoir importer la clef publique en machine-owner key (MOK)

sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der
-> demande de saisir un mot de passe, sera demander au prochain démarrage pour enregistrer la MOK

------[ 1.3.2 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)
=> ne pause pas de pb pour les bdd en terme de cow comme le recodsize du fs est bien supérieurà celui qu'elles utilises

------[ 1.3.3 - dataset

# un pool est un volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

------[ 1.3.4 - snapshot

cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF

--------[ 1.3.4.1 - service et timer

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF

cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable sanoid.timer
systemctl start sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

------[ 1.3.5 - backup

restic vers amazon s3

céer un compte de service restic sur aws avec les bonnes stratégie
https://restic.readthedocs.io/en/latest/080_examples.html

export AWS_DEFAULT_REGION="redacted"
export RESTIC_REPOSITORY="redacted"
export AWS_ACCESS_KEY_ID="redacted"
export AWS_SECRET_ACCESS_KEY="redacted"
export RESTIC_PASSWORD="redacted"

# initie le dépot
restic init

# première backup
restic backup /mnt/raid/nas/

--------[ 1.3.5.1 - script

# non automatisé car prend de la bande passante (impact non transparant aujourd'hui)
# curl l'ouput des commande vers ntfy pour travail en headless

cat << EOF > backup.sh
#!/bin/bash
export AWS_DEFAULT_REGION="redacted"
export RESTIC_REPOSITORY="redacted"
export AWS_ACCESS_KEY_ID="redacted"
export AWS_SECRET_ACCESS_KEY="redacted
export RESTIC_PASSWORD="redacted"
curl -X POST -d "$(/home/core/restic backup /mnt/raid/nas/ --exclude /mnt/raid/nas/jellyfin/data)" https://ntfy.lan/Backup
curl -X POST -d "$(/home/core/restic forget --keep-last 10)" https://ntfy.lan/Backup
curl -X POST -d "$(/home/core/restic prune)" https://ntfy.lan/Backup
EOF

----[ 1.4 - Certificat Authority

# créer la key
openssl genrsa -out lan.key 4096
# creér le root certificate (le common name indique le domaine sous lequel on signe, ici lan)
# le root certificate (lan.pem) est à installer sur toutes les machines qui ont besoin d'accéder aux services lan
openssl req -x509 -new -nodes -key lan.key -sha256 -days 3650 -out lan.pem -subj "/CN=lan"

# Pour la partie DNS ici on utilise pas un server DNS local mais NextDNS
# on fait une simple réécriture *.lan → 100.102.192.19 depuis Nextdns
# bien utiliser le DNS de tailscale avec nextdns en global nameserver, sans magicdns (obligatoire pour android)
# activer l'override des local DNS pour que toutes les requêtes DNS passe par tailscale (system-wide DNS sur tout les appareils)

# Pour chaque app création d'un certif signé par lan.
# Exemple pour searxng :

# créer la clef privé du website
openssl genrsa -out searxng.lan.key 4096

# créer le csr (common name = server hostname = searxng.lan)
openssl req -new -key searxng.lan.key -out searxng.lan.csr  -subj "/CN=searxng.lan"

# extension X509 V3
cat << EOF > searxng.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = searxng.lan
EOF

# le certificat final
openssl x509 -req -in searxng.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out searxng.lan.crt -days 3650 -sha256 -extfile searxng.lan.ext

# enregistrement de la pair en secret dans kubernetes pour application au niveau de l'ingress
kubectl create secret tls searxng-secret \
  --cert=./searxng.lan.crt \
  --key=./searxng.lan.key

=> voir pour cert-manager

----[ 1.5 - Gitea

# version self hosté de github
# en attendant fluxcd on applique un manifest manuellement
kubectl apply -f gitea.yaml

## intégrer openid (après keycloak en 2.2) :
depuis l'interface web aller sur administration du site > identié et accès > source d'authentification
créer une entrée Oauth2 avec un fournisseur OpenID connect

côté keycloak mettre client authentification pour credential

création d'un secret avec le pem :
kubectl create secret generic lan-pem --from-file=lan.pem
=> le mapper dans le conteneur en lecture seule

openid connect url de découverte :
https://keycloak.lan/realms/lan/.well-known/openid-configuration 

----[ 1.6 - FluxCD

# heberge les manifest sous git
# flux reconcilie le cluster avec l'état attendu sous git
# monitoring des repository avec maj auto

curl -s https://fluxcd.io/install.sh | sudo bash
. <(flux completion bash)
cp  /etc/rancher/k3s/k3s.yaml ~/.kube/config
flux check --pre

# bootstrap vers Gitea
flux bootstrap gitea --hostname=gitea.lan --owner=admin --repository=NixOS-k3s --private=false --personal=true --branch=main --path=k3s

cloner le repo
mettre les manifest dans /clusters/my-cluster
  attention : les manifests doivent spécifier le namespace à utiliser sinon timeout
push :
git add -A && \
git commit -m "add apps" && \
git push origin main

maj auto, possible de pull manuellement :
# flux reconcile kustomization flux-system --with-source

----[ 1.7 - maintenance

------[ 1.7.1 - du serveur sous maj auto

systemctl edit --force --full rpm-ostreed-upgrade-reboot.service

'
# workaround for missing reboot policy
# https://github.com/coreos/rpm-ostree/issues/2843
[Unit]
Description=rpm-ostree upgrade and reboot
ConditionPathExists=/run/ostree-booted

[Service]
Type=simple
ExecStart=/usr/bin/rpm-ostree upgrade --reboot
#StandardOutput=null
Restart=always
RestartSec=60s
'

cat << EOF > /etc/systemd/system/rpm-ostreed-upgrade-reboot.timer
[Unit]
Description=rpm-ostree upgrade and reboot trigger
ConditionPathExists=/run/ostree-booted

[Timer]
OnCalendar=*-*-* 4:00:00

[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable rpm-ostreed-upgrade-reboot.timer
systemctl start rpm-ostreed-upgrade-reboot.timer

reboot

------[ 1.7.1 - des pods

## Renovate
# setup les gitea actions :
-> gitea runner :

créer un runner dépuis l'interface web, récupérer le token pour le manifest du runner
besoin de l'image Docker in Docker (dind) comme on est sur kubernetes
utilisation de l'image de vegardit https://github.com/vegardit/docker-gitea-act-runner sinon pb de timeout réseau ce qui fait échoué les actions
nécessite securityContext: privileged: true
comme ici le CA et les nom de domaine ne sont pas resolvable sans configuration on tout nouveau monde de douleur s'ouvre à nous pour que les conteneurs dans le pod puisse se connecter :
	-> besoin de mapper un config.yaml demandans à docker d'utiliser le host network pour l'aspect DNS
	-> besoin de bind mount le pem venant du volume mount du pod pour fournir le certificat

=> vérifier que le runner apparait bien dans l'interface web

# rennovate

suivre :
https://about.gitea.com/resources/tutorials/use-gitea-and-renovate-bot-to-automatically-monitor-software-packages

créer un compte pour le bot renovate + un token
creér un renovate.json à la racine du dépôt sur lequel renovate doit agir + ajouter le comppte du bot en collaborateur
créer un dépôts pour la config de renovate
mettre dans ce dépôt le config.json et le workflows de renovate
mettre le token dans ce dépôt

# pb aléatoire du runner

git semble ignorer le system store alors que le pem est bien mapper
=> ajout d'env var supplémentaire au workflow NODE_EXTRA_CA_CERTS et GIT_SSL_CAINFO pointant vers le pem

# A aussi besoin d'un token GITHUB pour récupérer des changelogs

ajout d'une env var RENOVATE_GITHUB_COM_TOKEN et ajouter le token dans le dépôt

------[ 1.7.2 - du cluster

https://docs.k3s.io/upgrades
# k3s intègre une custom resource definition (CRD), les plans
# apparemment monitor et maj auto
prendre les plans server et agents
remplacer version par le channel field
mettre en latest

--[ 2 - apps

----[ 2.1 - Headlamp

# solution pour superviser le cluster k3s, consulter les logs des pod, entrer dans des shell...

# créer le compte de service
https://headlamp.dev/docs/latest/installation/#create-a-service-account-token

----[ 2.2 - keycloak

# Solution de SSO pour centraliser la gestion des identités et gestion d'accès, permet la customisation des flux d'authentification et d'utiliser des méthodes modernes (passwordless)

realm lan
créer les clients :
ocis web : ocis
desktop : xdXOt13JKxym1B1QcEncf2XDkLAexMBFwiT9j6EfhhHFJhs2KM9jbjTmf8JBXE69
android : e4rAsNUSIUs0lF4nbv9FmCeUkTlV9GdgTLDH1b5uie7syb90SzEVrbN7HIpmWJeD
paperless web : paperless

Access settings :
Root URL : https://owncloud.lan/*

Redirect URIs :
web : https://owncloud.lan/*
Desktop : http://127.0.0.1
Android : oc://android.owncloud.com

Web origins : *

Capability config :
toutes les Authentication flow cochables

# pour enregister les passkeys se co avec le compte user lan https://keycloak.lan/realms/lan/account/

# besoin de modifier dans l'authentification du realm l'entrée broswer (cookie -> idp redirector -> webauthn passwordless authenticator) pour prioriser le passwordless
revoir le bug du account-console qui était innaccessible (droit du compte, impersonation du compte admin realms master, bidouillage de l'entré account et account-console...)

# Pour utiliser des passkeys sous android besoin d'un paramétarge spécifique :
l'enregistrement d'une passkey ne marche pas sur firefox android (l'enregistrer depuis un chromium)
sur chrome avec les paramètres Authentification/policies/Webauthn Passwordless Policy:
Authenticator Attachment -> si platform = échec donc laissé en not specified
Require discoverable credential -> yes
    empêche l'enregistrement en locale de la passkey (cet appareil), mais celui ci ne marche pas (google password manager dit qu'aucune clef n'est dispo car justement pas discoverable...)

donc besoin d'une app tierse pour l'enregistrer :
-> bientôt keepassdx compatible
https://github.com/Kunzisoft/KeePassDX/issues/1421
-> proton pass en attendant ou google password manager

----[ 2.3 - Collabora

# suite office en ligne, s'utilise depuis ocis par WOPI

----[ 2.4 - ocis

# owncloud version moderne, légère, écrite en go

besoin d'une deuxième instance pour le wopiu et de plusieurs services :
wopi -> service entre instance de collab et collabora
registry -> com entre services, instance de collab et instance principale

Besoin d'un Content Security Policy (CSP) dans ocis-config pour autoriser Collabora et keycloak
csp.yaml :
---------
cat << EOF > csp.yaml
directives:
  child-src:
    - '''self'''
  connect-src:
    - '''self'''
    - 'blob:'
    - 'https://keycloak.lan'
  default-src:
    - '''none'''
  font-src:
    - '''self'''
  frame-ancestors:
    - '''self'''
  frame-src:
    - '''self'''
    - 'blob:'
    - 'https://embed.diagrams.net/'
    # In contrary to bash and docker the default is given after the | character
    - 'https://office.lan'
    # This is needed for the external-sites web extension when embedding sites
    - 'https://docs.opencloud.eu'
  img-src:
    - '''self'''
    - 'data:'
    - 'blob:'
    - 'https://raw.githubusercontent.com/opencloud-eu/awesome-apps/'
    # In contrary to bash and docker the default is given after the | character
    - 'https://office.lan'
  manifest-src:
    - '''self'''
  media-src:
    - '''self'''
  object-src:
    - '''self'''
    - 'blob:'
  script-src:
    - '''self'''
    - '''unsafe-inline'''
  style-src:
    - '''self'''
    - '''unsafe-inline'''
EOF

ici 2 créations de cert ssl comme 2 instances
---------------------------------------------
openssl genrsa -out owncloud.lan.key 4096
openssl req -new -key owncloud.lan.key -out owncloud.lan.csr  -subj "/CN=owncloud.lan"
cat << EOF > owncloud.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = owncloud.lan
EOF
openssl x509 -req -in owncloud.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out owncloud.lan.crt -days 3650 -sha256 -extfile owncloud.lan.ext

secret :
kubectl create secret tls owncloud-secret \
  --cert=./owncloud.lan.crt \
  --key=./owncloud.lan.key

openssl genrsa -out wopi.lan.key 4096
openssl req -new -key wopi.lan.key -out wopi.lan.csr  -subj "/CN=wopi.lan"
cat << EOF > wopi.lan.ext
authorityKeyIdentifier=keyid,issuer
basicConstraints=CA:FALSE
keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = wopi.lan
EOF
openssl x509 -req -in wopi.lan.csr -CA lan.pem -CAkey lan.key \
-CAcreateserial -out wopi.lan.crt -days 3650 -sha256 -extfile wopi.lan.ext

secret :
kubectl create secret tls wopi-secret \
  --cert=./wopi.lan.crt \
  --key=./wopi.lan.key

----[ 2.5 - searxng

# metamoteur de recherche pour des resultats plus opti

----[ 2.6 - navidrome

# serveur de musique avec l'API subsonic

----[ 2.7 - paperless-ngx

# Solution d'archivage de documents

# créer le compte admin
python3 manage.py createsuperuser

----[ 2.8 - calibre-web-automated

# bibliothèque en ligne

----{ 2.9 - jellyfin

# serveur multimédia

----[ 2.10 - overleaf

# editeur LaTex en ligne

----[ 2.11 - Trilium Next

# Application de prise de note, Gestion des connaissances dispo par interface web

----[ 2.12 - ntfy

# pub-sub notification service pour le monitoring du ZFS et du SMART des SSD
ZFS besoin de paramétrer /etc/zfs/zed.d/zed.rc :
url ntfy : https://ntfy.lan
sujet : ZFS
verbosité : 1
-> notif envoyé à https://ntfy.lan/ZFS

----------------- notes

Vérif de l'image utilisé par l'OS : 
rpm-ostree status

logs :
sudo kubectl logs pod-name

shell :
sudo kubectl exec --stdin --tty pod -- /bin/sh

toutes les ressources :
kubectl get 

voir le détails des tag d'images observé :
kubectl -n flux-system describe imagerepositories searxng

voir le status du traking des images :
flux get images all --all-namespaces

debug les reconciliation :
flux get ks

flux reconcile kustomization flux-system --with-source

=> pb de node unschedulable
kubectl patch nodes searxng.lan --patch '{"spec":{"unschedulable": false}}'

au niveau du dns:
les container on connaissances des services côté non de domaine (ex : "redis://redis:6379" renvoi vers le service redis)
pour modifier le fichier host, notamment pour une connaissance d'un domaine hors k3s on peut uitliser hostaliases
exemple : 
    spec:
      hostAliases:
      - ip: "192.168.1.200"
        hostnames:
          - "owncloud.lan"
          - "office.lan"
