+-------------------------------------------------------------------------+
|=----------------=[ Immutable NAS & hosting platform ]=-----------------=|
|-------------------------------------------------------------------------|
|=---------------------=[ uCore, docker, RAID ZFS ]=---------------------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation
	1.1 - rebase ucore
	1.2 - maj auto
2 - RAID ZFS
	2.1 - SecureBoot avec kmod ZFS
	2.2 - pool mirroir
	2.3 - dataset
	2.4 - snapshot journalière sanoid
	2.5 - systemd timers
3 - Searxng
	3.1 - SSL
	3.2 - conf haproxy
	3.3 - macvlan
	3.4 - firewalld
	3.5 - docker-compose
4 - owncloud infini scale (oCIS)
	4.1 - Infinite Scale Setup
	4.2 - Infinite Scale deploiement
5 - homepage
	5.1 - settings
	5.2 - bookmarks
	5.3 - services
	5.4 - widgets
	5.5 - docker.yaml
	5.6 - docker run
6 - navidrome
	
--[ 0 - Introduction

# Fedora CoreOS (FCOS) rebase en uCore
# Immutable, atomic auto update, secure (SELinux + SecureBoot + immutable)
# uCore-ZFS avec module signé et batteries included (docker, sanoid, firewalld, ...)
# home drive avec owncloud infini scale (version cloud native)
# data sous un RAID ZFS avec sbapshot journalière
# hosting de searxng sous macvlan

--[ 1 - installation

----[ 1.1 - rebase ucore

# créer un fichier butane qui comprend =  clef ssh, conf réseaux, maj et reboot auto + rebase uCore en stable-zfs
-> https://github.com/ublue-os/ucore?tab=readme-ov-file#auto-rebase-install + https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/
butane --pretty --strict .\truc.bu > .\truc.ign

mettre le .ign sur une clef usb et depuis le live environnement
(hyper-v mkfs.vfat sur le vhdx puis copy/paste)
sudo mount /dev/sda /mnt
cp /mnt/truc.ign ~/truc.ign
sudo umount /mnt

sudo coreos-installer install /dev/sda  --ignition-file /truc.ign

----[ 1.2 - maj auto

sudo systemctl edit --force --full rpm-ostreed-upgrade-reboot.service

'
# workaround for missing reboot policy
# https://github.com/coreos/rpm-ostree/issues/2843
[Unit]
Description=rpm-ostree upgrade and reboot
ConditionPathExists=/run/ostree-booted

[Service]
Type=simple
ExecStart=/usr/bin/rpm-ostree upgrade --reboot
#StandardOutput=null
'

sudo bash -c 'cat << EOF > /etc/systemd/system/rpm-ostreed-upgrade-reboot.timer
[Unit]
Description=rpm-ostree upgrade and reboot trigger
ConditionPathExists=/run/ostree-booted

[Timer]
OnBootSec=1h
OnUnitInactiveSec=1d

[Install]
WantedBy=timers.target
EOF
'

sudo systemctl daemon-reload
sudo systemctl enable rpm-ostreed-upgrade-reboot.timer
sudo systemctl start rpm-ostreed-upgrade-reboot.timer

--[ 2 - RAID ZFS

# création du RAID1 par zfs
# snapshot journalière par sanoid avec 30 jours de rétention

----[ 2.1 - SecureBoot avec kmod ZFS

# ZFS ne marchera pas par défaut sous secureboot avant d'avoir importer la clef publique en machine-owner key (MOK)

sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der
-> demande de saisir un mot de passe, sera demander au prochain démarrage pour enregistrer la MOK

----[ 2.2 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)

----[ 2.3 - dataset

# un pool est volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

----[ 2.4 - snapshot journalière sanoid

sudo bash -c 'cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF
'

----[ 2.5 - systemd timers

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF
'

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF
'

sudo systemctl daemon-reload
sudo systemctl enable sanoid.timer
sudo systemctl start sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

--[ 3 - docker 

# backend searxng proxifié dans gluetun
# frontend haproxy (macvlan)

sudo usermod -aG docker core
sudo chown -R core /mnt/raid
sudo systemctl enable docker
sudo systemctl start docker

----[ 3.1 - SSL

mkdir /mnt/raid/nas/ssl
mkdir /mnt/raid/nas/ssl/private
openssl genrsa -out /mnt/raid/nas/ssl/private/searxng.key 4096
openssl req -new -key /mnt/raid/nas/ssl/private/searxng.key \
  -out /mnt/raid/nas/ssl/private/searxng.csr
openssl x509 -sha256 -req -days 365 \
  -in /mnt/raid/nas/ssl/private/searxng.csr \
  -signkey /mnt/raid/nas/ssl/private/searxng.key \
  -out /mnt/raid/nas/ssl/searxng.crt
  
# creer un PEM pour haproxy :
cat /mnt/raid/nas/ssl/searxng.crt /mnt/raid/nas/ssl/private/searxng.key > /mnt/raid/nas/ssl/private/searxng.pem

# pour l'installer les cert coté client sous android, creer un PEM au format pkcs12 :
openssl pkcs12 -export -in server.crt -inkey server.key -out android.p12

----[ 3.2 - conf haproxy

mkdir /mnt/raid/nas/haproxy
cat << EOF > /mnt/raid/nas/haproxy/haproxy.cfg
frontend www
  bind :443 ssl crt /etc/ssl/private/searxng.pem
  default_backend searxng

backend searxng
  mode http
  server searxng gluetun:8080
EOF

# le DNS interne de docker permet la conf directement avec le nom des conteneurs
# 8080 = port d’écoute par défaut de searxng qui utilisera gluetun comme proxy

----[ 3.3 - macvlan

docker network create -d macvlan \
--subnet=192.168.1.0/24 \
--gateway=192.168.1.1 \
-o macvlan_mode=bridge \
-o parent=eth0 macvlan

# le réseau macvlan est accessible depuis l’extérieur mais pas depuis l'hôte
# ajouter un interface a lié a eth0 si besoin (disparaît au prochain démarrage)

sudo ip link set dev eth0 promisc on
sudo ip link add  docker-external link eth0 type macvlan  mode bridge
sudo ip link set  docker-external up
sudo ip route add 172.19.182.213/32 dev  docker-external

----[ 3.4 - firewalld

sudo firewall-cmd --add-service=http --permanent
sudo firewall-cmd --add-service=https --permanent

----[ 3.5 - docker-compose

cat << EOF > /mnt/nas/docker-compose.yml
services:
  gluetun:
   image: qmcgaw/gluetun
   container_name: gluetun
   cap_add:
    - NET_ADMIN
   environment:
    - VPN_SERVICE_PROVIDER=custom
    - VPN_TYPE=wireguard
    - VPN_ENDPOINT_IP=
    - VPN_ENDPOINT_PORT=
    - WIREGUARD_PUBLIC_KEY=
    - WIREGUARD_PRIVATE_KEY=
    - WIREGUARD_ADDRESSES=
   restart: always

  searxng:
   image: searxng/searxng
   network_mode: "service:gluetun"
   container_name: searxng
   restart: always

  haproxy:
   image: haproxy
   container_name: haproxy
   networks:
    macvlan: #frontend
     ipv4_address: 192.168.1.201
	default: #backend
   ports:
    - 443:443
   volumes:
      - /mnt/raid/nas/haproxy:/usr/local/etc/haproxy:Z # Z pour selinux sinon fini bloque
	  - /mnt/raid/nas/ssl/private:/etc/ssl/private:Z
   restart: always

networks:
  macvlan:
    external: true
EOF

docker-compose up -d 

# maj du docker compose

docker-compose pull && docker-compose up -d
 
--[ 4 - owncloud infini scale (oCIS)

# owncloud version cloud-native sous micro-services (plus de php, sql...)
https://doc.owncloud.com/ocis/next/deployment/container/container-setup.html

----[ 4.1 - Infinite Scale setup

docker pull owncloud/ocis

mkdir -p /mnt/raid/nas/ocis/ocis-config
mkdir -p /mnt/raid/nas/ocis/ocis-data
sudo chown -Rfv 1000:1000 /mnt/raid/nas/ocis/

# la doc utilise mount mais ce bind mount be supporte pas les labels selinux -> utiliser les volumes

sudo docker run --rm -it \
    --volume /mnt/raid/nas/ocis/ocis-config:/etc/ocis:Z \
    owncloud/ocis init

----[ 4.2 - Infinite Scale deploiement

# OCIS_INSECURE=true -> automatically generated and self-signed certificates
# laisser les port en 9200 sinon pb de dialogues entre services

sudo docker run -d \
   --name ocis_runtime \
   -p 9200:9200 \
   --volume /mnt/raid/nas/ocis/ocis-config:/etc/ocis:Z \
   --volume /mnt/raid/nas/ocis/ocis-data:/var/lib/ocis:Z \
   -e OCIS_INSECURE=true \
   -e PROXY_HTTP_ADDR=0.0.0.0:9200 \
   -e OCIS_URL=https://192.168.1.200:9200 \
   --restart always \
   owncloud/ocis
   
pour exécuter les commandes d'admin :

docker exec -it <container-id> sh
ocis --help

--[ 5 - homepage

# Pour afficher les stats et services de l'intranet

mkdir -p /mnt/raid/nas/homepage/icon
cd /mnt/raid/nas/homepage/icon
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/openwrt.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/owncloud.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/searxng.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/gluetun.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/haproxy.svg
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/png/homepage.png
curl -O https://cdn.jsdelivr.net/gh/walkxcode/dashboard-icons/svg/navidrome.svg

----[ 5.1 - settings

cat << EOF > /mnt/raid/nas/homepage/settings.yaml
---
background:
  image: https://images3.alphacoders.com/132/1324687.jpg
  brightness: 50
  saturate: 50
color: slate
layout:
  stats-firewall:
    style: row
    columns: 2
  Docker:
    style: row
    columns: 3
showStats: true
EOF

----[ 5.2 - bookmarks

cat << EOF > /mnt/raid/nas/homepage/bookmarks.yaml
---
- Intranet:
    - ownCloud:
        - icon: owncloud.svg
          href: https://192.168.1.200:9200
    - OpenWRT:
        - icon: openwrt.svg
          href: https://192.168.1.1
    - SearXNG:
        - icon: searxng.svg
          href: https://192.168.1.201
    - Navidrone:
        - icon: navidrome.svg
          href: http://192.168.1.200:4533
EOF

----[ 5.3 - services

services.yaml
cat << EOF > /mnt/raid/nas/homepage/services.yaml
---
- OpenWRT:
    - Upstream:
        icon: openwrt.svg
        widget:
          type: openwrt
          url: https://192.168.1.1
          username: homepage
          password: starwars
          interfaceName: eth1
    - Info:
        icon: openwrt.svg
        widget:
          type: openwrt
          url: https://192.168.1.1
          username: homepage
          password: starwars

- Docker:
    - HAProxy:
        icon: haproxy.svg
        description: reverse-proxy pour tls
        server: my-docker
        container: haproxy
    - SearXNG:
        icon: searxng.svg
        description: meta-moteur de recherche
        href: "https://192.168.1.201"
        server: my-docker
        container: searxng
    - Gluetun:
        icon: gluetun.svg
        description: proxy wireguard pour container
        server: my-docker
        container: gluetun
    - OCIS:
        icon: owncloud.svg
        description: partage de fichier
        href: "https://192.168.1.200:9200"
        server: my-docker
        container: ocis_runtime
    - homepage:
        icon: homepage.png
        description: intranet
        href: "http:192.168.1.200"
        server: my-docker
        container: homepage
    - navidrome:
        icon: navidrome.svg
        description: serveur de musique
        href: "http:192.168.1.200:4533"
        server: my-docker
        container: navidrome
EOF

----[ 5.4 - widgets

cat << EOF > /mnt/raid/nas/homepage/widgets.yaml
---
- resources:
    cpu: true
    memory: true
    cputemp: true
    tempmin: 0 # optional, minimum cpu temp
    tempmax: 100 # optional, maximum cpu temp
    uptime: true
    refresh: 3000 # optional, in ms
EOF

----[ 5.5 - docker yaml

cat << EOF > /mnt/raid/nas/homepage/docker.yaml
---
my-docker:
  socket: /var/run/docker.sock
EOF

----[ 5.6 - docker run

sudo docker run -d \
	--name homepage \
	-p 80:3000 \
	--security-opt label=disable \
	-v /mnt/raid/nas/homepage:/app/config:Z \
	-v /var/run/docker.sock:/var/run/docker.sock:Z \
	-v /mnt/raid/nas/homepage/icon:/app/public/icons:Z \
	--restart always \
	ghcr.io/gethomepage/homepage:latest

--[ 6 - navidrome

mkdir /mnt/raid/nas/music/music-data
mkdir /mnt/raid/nas/music/music-navidrome

docker run -d \
   --name navidrome \
   --restart=always \
   --user $(id -u):$(id -g) \
   -v /mnt/raid/nas/music/music-data:/music:Z \
   -v /mnt/raid/nas/music/music-navidrome:/data:Z \
   -p 4533:4533 \
   -e ND_LOGLEVEL=info \
   deluan/navidrome:latest

# pour gérer les flag pour les fichiers flac on peut installer le paquets flag dans un container distrobox 
# pour indiquer les musiques font parties d;une compilation :

metaflac --set-tag=COMPILATION=1 *.flac

+-[ EOF ]=----------------------------------------------------------------+
