+-------------------------------------------------------------------------+
|=----------------=[ Immutable NAS & hosting platform ]=-----------------=|
|-------------------------------------------------------------------------|
|=---------------------=[ uCore, docker, RAID ZFS ]=---------------------=|
+-------------------------------------------------------------------------+

--[ Table of contents

0 - Introduction
1 - Installation/rebase
2 - RAID ZFS
	2.1 - SecureBoot avec kmod ZFS
	2.2 - pool mirroir
	2.3 - dataset
	2.4 - snapshot journalière sanoid
	2.5 - systemd timers
3 - Searxng
	3.1 - SSL
	3.2 - conf haproxy
	3.3 - macvlan
	3.4 - firewalld
	3.5 - docker-compose
4 - owncloud infini scale (oCIS)
	4.1 - Infinite Scale Setup
	4.2 - Infinite Scale deploiement
	
--[ 0 - Introduction

# Fedora CoreOS (FCOS) rebase en uCore
# Immutable, atomic auto update, secure (SELinux + SecureBoot + immutable)
# uCore-ZFS avec module signé et batteries included (docker, sanoid, firewalld, ...)
# home drive avec owncloud infini scale (version cloud native)
# data sous un RAID ZFS avec sbapshot journalière
# hosting de searxng sous macvlan

--[ 1 - install + rebase ucore

# créer un fichier butane qui comprend =  clef ssh, conf réseaux, maj et reboot auto + rebase uCore en stable-zfs
-> https://github.com/ublue-os/ucore?tab=readme-ov-file#auto-rebase-install + https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/
butane --pretty --strict .\truc.bu > .\truc.ign

mettre le .ign sur une clef usb et depuis le live environnement
(hyper-v mkfs.vfat sur le vhdx puis copy/paste)
sudo mount /dev/sda /mnt
cp /mnt/truc.ign ~/truc.ign
sudo umount /mnt

sudo coreos-installer install /dev/sda  --ignition-file /truc.ign

--[ 2 - RAID ZFS

# création du RAID1 par zfs
# snapshot journalière par sanoid avec 30 jours de rétention

----[ 2.1 - SecureBoot avec kmod ZFS

# ZFS ne marchera pas par défaut sous secureboot avant d'avoir importer la clef publique en machine-owner key (MOK)

sudo mokutil --import /etc/pki/akmods/certs/akmods-ublue.der
-> demande de saisir un mot de passe, sera demander au prochain démarrage pour enregistrer la MOK

----[ 2.2 - pool mirroir

https://jrs-s.net/2018/08/17/zfs-tuning-cheat-sheet/

sudo zpool create -o ashift=12 -O xattr=sa -O compression=lz4 -O atime=off -O recordsize=1M -m /mnt/raid raid mirror /dev/sdb /dev/sdc

# ashift = Ashift tells ZFS what the underlying physical block size your disks use is
## ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs).

# xattr = Sets Linux eXtended ATTRibutes directly in the inodes, rather than as tiny little files in special hidden folders.
## This can have a significant performance impact on datasets with lots of files in them, particularly if SELinux is in play.

# compression = Compression defaults to off
## lz4 plus rapide, zstd compresse davantage

# atime = If atime is on – which it is by default – your system has to update the “Accessed” attribute of every file every time you look at it. 
## This can easily double the IOPS load on a system all by itself. Ici osef

# recordsize = you want to match the recordsize to the size of the reads or writes you’re going to be digging out of / cramming into those large files.
## 1M for reading and writing in fairly large chunks (jpeg, movies...)

----[ 2.3 - dataset

# un pool est volume, afin bénéficier des capacités du fs (cow, checksum, snap...) il faut créer un dataset dans ce pool

sudo zfs create raid/nas

# zfs get all raid/nas pour voir les attribues du pool/dataset
## sauf indication contraire le dataset hérite des attribues du pool

## dossier accessible sous /mnt/raid/nas

----[ 2.4 - snapshot journalière sanoid

sudo bash -c 'cat << EOF > /etc/sanoid/sanoid.conf
[raid/nas]
	use_template = production

#############################
# templates below this line #
#############################

[template_production]
        frequently = 0
        hourly = 0
        daily = 30
        monthly = 0
        yearly = 0
        autosnap = yes
        autoprune = yes
EOF
'

----[ 2.5 - systemd timers

# pas cron sur le system pour lancer sanoid donc utiliser systemd Timers
https://wiki.archlinux.org/title/Systemd/Timers

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.service
[Unit]
Description=Snapshot ZFS Pool
Requires=zfs.target
After=zfs.target
ConditionFileNotEmpty=/etc/sanoid/sanoid.conf

[Service]
Environment=TZ=UTC
Type=oneshot
ExecStart=/usr/sbin/sanoid
EOF
'

sudo bash -c 'cat << EOF > /etc/systemd/system/sanoid.timer
[Unit]
Description=Run Sanoid Every 15 Minutes

[Timer]
OnCalendar=*:0/15
Persistent=true

[Install]
WantedBy=timers.target
EOF
'

sudo systemctl daemon-reload
sudo systemctl enable sanoid.timer

# pour lister les timers 
systemctl list-timers

# pour lister les snapshot
zfs list -t snapshot

--[ 3 - docker 

# backend searxng proxifié dans gluetun
# frontend haproxy (macvlan)

sudo usermod -aG docker core

----[ 3.1 - SSL

mkdir /mnt/nas/ssl
mkdir /mnt/nas/ssl/private
sudo openssl genrsa -out /mnt/nas/ssl/private/server.key 4096
sudo openssl req -new -key /mnt/nas/ssl/private/server.key \
  -out /mnt/nas/ssl/private/server.csr
sudo openssl x509 -sha256 -req -days 365 \
  -in /mnt/nas/ssl/private/server.csr \
  -signkey /mnt/nas/ssl/private/server.key \
  -out /mnt/nas/ssl/server.crt
  
# creer un PEM pour haproxy :
cat /mnt/nas/ssl/server.crt /mnt/nas/ssl/private/server.key > /mnt/nas/ssl/private/haproxy.pem

# pour l'installer les cert coté client sous android, creer un PEM au format pkcs12 :
openssl pkcs12 -export -in server.crt -inkey server.key -out android.p12

----[ 3.2 - conf haproxy

mkdir /mnt/nas/haproxy
cat << EOF > /mnt/nas/haproxy/haproxy.cfg
frontend www
  bind :443 ssl crt /etc/ssl/private/haproxy.pem
  default_backend searxng

backend searxng
  mode http
  server searxng gluetun:8080
EOF

# le DNS interne de docker permet la conf directement avec le nom des conteneurs
# 8080 = port d’écoute par défaut de searxng qui utilisera gluetun comme proxy

----[ 3.3 - macvlan

docker network create -d macvlan \
--subnet=192.168.1.0/24 \
--gateway=192.168.1.1 \
-o macvlan_mode=bridge \
-o parent=eth0 macvlan

# le réseau macvlan est accessible depuis l’extérieur mais pas depuis l'hôte
# ajouter un interface a lié a eth0 si besoin (disparaît au prochain démarrage)

sudo ip link set dev eth0 promisc on
sudo ip link add  docker-external link eth0 type macvlan  mode bridge
sudo ip link set  docker-external up
sudo ip route add 172.19.182.213/32 dev  docker-external

----[ 3.4 - firewalld

sudo firewall-cmd --add-service=http --permanent
sudo firewall-cmd --add-service=https --permanent

----[ 3.5 - docker-compose

cat << EOF > /mnt/nas/docker-compose.yml
services:
  gluetun:
   image: qmcgaw/gluetun
   container_name: gluetun
   cap_add:
    - NET_ADMIN
   environment:
    - VPN_SERVICE_PROVIDER=custom
    - VPN_TYPE=wireguard
    - VPN_ENDPOINT_IP=
    - VPN_ENDPOINT_PORT=
    - WIREGUARD_PUBLIC_KEY=
    - WIREGUARD_PRIVATE_KEY=
    - WIREGUARD_ADDRESSES=
   restart: always

  searxng:
   image: searxng/searxng
   network_mode: "service:gluetun"
   container_name: searxng
   restart: always

  haproxy:
   image: haproxy
   container_name: haproxy
   networks:
    macvlan: #frontend
     ipv4_address: 
	default: #backend
   ports:
    - 443:443
   volumes:
      - /mnt/nas/haproxy:/usr/local/etc/haproxy:Z # Z pour selinux sinon fini bloque
	  - /mnt/nas/ssl:/etc/ssl/private:Z
   restart: always

networks:
  macvlan:
    external: true
EOF

docker-compose up -d 
 
--[ 4 - owncloud infini scale (oCIS)

# owncloud version cloud-native sous micro-services (plus de php, sql...)
https://doc.owncloud.com/ocis/next/deployment/container/container-setup.html

----[ 4.1 - Infinite Scale setup

docker pull owncloud/ocis

mkdir -p /mnt/nas/ocis/ocis-config
mkdir -p /mnt/nas/ocis/ocis-data
sudo chown -Rfv 1000:1000 /mnt/nas/ocis/

# la doc utilise mount mais ce bind mount be supporte pas les labels selinux -> utiliser les volumes

docker run --rm -it \
    --volume /mnt/nas/nas/ocis/ocis-config:/etc/ocis:Z \
    owncloud/ocis init

----[ 4.2 - Infinite Scale deploiement

# OCIS_INSECURE=true -> automatically generated and self-signed certificates
# laisser les port en 9200 sinon pb de dialogues entre services

docker run -d \
   --name ocis_runtime \
   -p 9200:9200 \
   --volume /mnt/nas/nas/ocis/ocis-config:/etc/ocis:Z \
   --volume /mnt/nas/nas/ocis/ocis-data:/var/lib/ocis:Z \
   -e OCIS_INSECURE=true \
   -e PROXY_HTTP_ADDR=0.0.0.0:9200 \
   -e OCIS_URL=https://ip:9200 \
   --restart always \
   owncloud/ocis
   
pour exécuter les commandes d'admin :

docker exec -it <container-id> sh
ocis --help

+-[ EOF ]=----------------------------------------------------------------+
